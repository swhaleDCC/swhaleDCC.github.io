<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Mac如何创建虚拟机 | VirtualBox + Vagrant]]></title>
    <url>%2F2021%2F02%2F17%2Fmac%E5%AE%89%E8%A3%85%E8%99%9A%E6%8B%9F%E6%9C%BA%2F</url>
    <content type="text"><![CDATA[记录在Mac上安装虚拟机的过程。 安装软件VirtualBox VirtualBox 是由德国 Innotek 公司开发，由Sun Microsystems公司出品的软件，使用Qt编写，在 Sun 被 Oracle 收购后正式更名成 Oracle VM VirtualBox。VirtualBox 和 VMWare 是同类型软件，开源，可以在当前运行的系统上构建出一台虚拟电脑。 下载主程序： https://www.virtualbox.org/wiki/Downloads 下拉网页下载扩展包，有些高级特性，比如 USB 3.0 等需要扩展包的支持： 在 Windows 系统中使用 VirtualBox 需开机进入BIOS开启CPU虚拟化。 Mac则不同： 首先看硬件是否支持，如果输出中有【VMX】，说明支持： 1sysctl -a | grep machdep.cpu.features 然后再次输入，如果输出为1那么说明支持【VT-x VT-d】（不支持的话只能更新你的系统了）： 1sysctl kern.hv_support 如果无法开启，那么要设置，输入下面的命令，输入密码，即可设置好，重启Mac就可以打开： 1sudo nvram boot-args=”kext-dev-mode=1 Vagrant vagrant是一个工具，用于创建和部署虚拟化开发环境的。VirtualBox会开放一个创建虚拟机的接口，Vagrant会利用这个接口创建虚拟机，并且通过Vagrant来管理，配置和自动安装虚拟机。 下载链接： https://www.vagrantup.com/downloads Vagrant 是没有图形界面的，所以安装完成后也没有桌面快捷方式。可以在终端执行 vagrant version 检查是否安装成功： 创建虚拟机官方下载虚拟机镜像使用 Vagrant 创建虚拟机时，需要指定一个镜像，也就是 box。开始这个 box 不存在，所以 Vagrant 会先从网上下载，然后缓存在本地目录中。 Vagrant 官方镜像仓库： https://app.vagrantup.com/boxes/search 在镜像仓库中选择要下载的版本，点击进入后可以看到如下图所示的how to use： 在终端执行： 其他方式下载.box如果官方默认下载比较慢，可以在其它地方下载到基础镜像，然后按照自己的需要重置。 CentOS 的镜像下载网站是： http://cloud.centos.org/centos/ 。在其中选择自己想要下载的版本，列表中有一个 vagrant 目录，里面是专门为 vagrant 构建的镜像。选择其中的 .box 后缀的文件下载即可。这里可以使用下载工具，以较快的速度下载下来。 Ubuntu 的镜像下载网站是： http://cloud-images.ubuntu.com/ 。同样先选择想要的版本，然后选择针对 vagrant 的 .box 文件即可。如果这里官网的速度较慢，还可以从 清华大学的镜像站 下载。 接下来需要将下载后的 .box 文件添加到 vagrant 中。终端执行: 1vagrant box add .box文件的路径 --name 自定义镜像的名称 启动虚拟机 参考： https://zhuanlan.zhihu.com/p/259833884 https://www.jianshu.com/p/0cabd5072b86]]></content>
      <categories>
        <category>Mac</category>
      </categories>
      <tags>
        <tag>virtualbox</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[支付宝自动收能量]]></title>
    <url>%2F2021%2F02%2F15%2F%E6%94%AF%E4%BB%98%E5%AE%9D%E8%87%AA%E5%8A%A8%E6%94%B6%E8%83%BD%E9%87%8F%2F</url>
    <content type="text"><![CDATA[参考 https://mp.weixin.qq.com/s/shyTtZGWvAgdtXFewUaMzQ ，实现了蚂蚁森林自动收能量。 运行环境 macOS Big Sur 11.2 小米10的手机 准备工作安装uiautomator2安装方法： 1pip install --upgrade --pre uiautomator2 UiAutomator 是 Google 提供的用来做安卓自动化测试的一个 Java 库，可以获取屏幕上任意一个 APP 的任意一个控件属性，并对其进行任意操作。Uiautomator2 是在 Uiautomator 之上的 Python 的接口封装，简单来说 Uiautomator2 可以看到手机当前屏幕上有哪些控件，其坐标，并且还可以模拟点击。 开启开发者模式和USB调试手机接入电脑前首先需要开启开发者模式，并开启USB调试，USB安装 (以小米10为例：如下图)，这样才能保证uiautomator2有足够的权限操作你的手机。 第一次使用过uiautomator2后，它会在你手机上安装ATX这个应用，打开这个应用你就可以之后通过无线的方式操作你的手机了。 安装adbMac可以直接使用brew命令安装adb，需要先安装Homebrew： 1/usr/bin/ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot; 安装adb： 1brew install android-platform-tools 安装成功执行: 1adb devices 如下图所示即adb安装成功，并且手机和电脑连接成功。￼ 注：如果adb没有安装会报错 RuntimeError: No adb exe could be found. Install adb on your system。 具体实现代码实现（具体含义见注释）： 12345678910111213141516171819202122232425262728293031323334353637383940import uiautomator2 as u2import timeimport random# 连接手机d = u2.connect() # 第一次使用有线连接，手机需要插电脑上，并开启开发者模式和USB调试 # d = u2.connect(&quot;xxx.xxx.xxx.xxx&quot;) # 除了第一次以外，可以通过无线连接，电脑和手机需要在同一个局域网内# 打开支付宝d.app_start(&quot;com.eg.android.AlipayGphone&quot;) # Uiautomator2可以直接通过应用包名调起应用time.sleep(2) # 休眠2s等待支付宝完全启动# 打开蚂蚁森林d(text=&quot;蚂蚁森林&quot;).click() # Uiautomators2可以直接点击屏幕某个文字的位置，所以需要把蚂蚁森林放的支付宝首页，这样打开支付宝后就可以直接定位到蚂蚁森林的位置了time.sleep(5) # 休眠5s等待支付宝加载完 # 寻找能量def collectEnergy(cnt): print(&quot;开始第%d次偷能量！&quot; % cnt) # 扫描点击有能量出现的区域 for x in range(150,1000,150): for y in range(600,900,150): d.long_click(x + random.randint(10,20), y + random.randint(10,20), 0.1) time.sleep(0.01) if cnt != 1: d.click(536,1816)# 偷能量cnt = 1while True: collectEnergy(cnt) a = d.xpath(&quot;//*[@resource-id=&apos;J_tree_dialog_wrap&apos;]&quot;).get().bounds # 把所有能量可能出现的位置都扫一遍 d.click(1000, a[3]-80) # 点击找能量按钮跳到下一个人那继续偷 if d.xpath(&apos;//*[@text=&quot;返回我的森林&quot;]&apos;).click_exists(timeout=2.0): # 如果页面出现了“返回我的森林”说明已经没有能量可偷了，结束 break cnt += 1print(&quot;###结束###&quot;)# 退出支付宝d.app_stop(&quot;com.eg.android.AlipayGphone&quot;) 注：第一次电脑连接手机需要注释掉第七行，取消注释第六行，手机会自动安装ATX，后面就可以通过ATX上面显示的ip无线连接手机。手机上的ATX如图所示：]]></content>
  </entry>
  <entry>
    <title><![CDATA[「面试」网易后台开发实习]]></title>
    <url>%2F2021%2F02%2F05%2F%E3%80%8C%E9%9D%A2%E8%AF%95%E3%80%8D%E7%BD%91%E6%98%93%E5%90%8E%E5%8F%B0%E5%BC%80%E5%8F%91%E5%AE%9E%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[一面，大概五十多分钟。 自我介绍 之前实习做的是xx，实验室的方向是xx，为什么想来做后台开发？答：吧啦吧啦…..对后台开发感兴趣…….未来的规划就是做后台开发。 最擅长的编程语言是什么？答：Java。 Java的集合介绍一下？答：list（ArrayList、LinkedList、Vector）、set（HashSet、TreeSet）、map（HashMap、LinkedHashMap、hashtable）。其实一开始面试很紧张，吧啦吧啦说了一通这些集合。 Set怎么判断是否有重复的？答：根据hashcode()和equals方法()……. 为什么需要hashcode()方法？答：当时真的不知道… Final关键字介绍一下基本用法？答：修饰类、方法、变量…… 重载和重写的区别？答：重载是一个类里面有多个名称相同但是参数列表不同的方法，重写是子类中对父类中的方法进行重新编写…. 几种创建线程的方法？答：四种，继承Thread类，实现Runnable接口，实现Callable接口，线程池。 前三种方法做个对比？答：不知道，忘记了…. 线程有几种状态？答：五种，创建、就绪、运行、阻塞、死亡…. 数据库用的什么？答：MySQL。 解释一下事务是什么？答：事务是数据库执行业务的最小单位，是一个操作序列，构成事务的所有操作要么全都成功执行，要么全都撤回，不管事务是否执行成功，数据库总能保持一致性状态…..（这里提了一下ACID hhh 因为上个月分布式数据库期末考试刚背了这些，果然：） 解释一下什么是ACID特性？答：原子性、一致性、持久性、隔离性…… 有两个表：员工表：t1：id, name, pid(主管的id)工资表：s1：id, salary找到比主管工资高的员工的name？这个题和力扣一道题极像！！！上学期刚刷过！！！但是我忘了！！！哭死链接：https://leetcode-cn.com/problems/employees-earning-more-than-their-managers/面试官问的稍微有一点不同，我代码没写出来，我觉得先把两个表连接起来，但是要用哪个关键词做连接忘记了。。。。 有一个Double数组，所有元素都是正数，求连续子数组最大乘积？这个也是和力扣的求连续子数组的最大和极像！！！我也是一开始代码没写出来，真的是一到面试脑子就成浆糊，面试官来来回回提醒我三次！！！链接：https://leetcode-cn.com/problems/lian-xu-zi-shu-zu-de-zui-da-he-lcof/ 如果数组里面有负数和0呢？说一下思路不用写代码。？？？求解？？？]]></content>
      <categories>
        <category>面试</category>
      </categories>
      <tags>
        <tag>网易</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[22岁-愿以诚挚之心，领岁月之教诲]]></title>
    <url>%2F2020%2F11%2F06%2F22-%E6%84%BF%E4%BB%A5%E8%AF%9A%E6%8C%9A%E4%B9%8B%E5%BF%83%EF%BC%8C%E9%A2%86%E5%B2%81%E6%9C%88%E4%B9%8B%E6%95%99%E8%AF%B2%2F</url>
    <content type="text"><![CDATA[过完这个周末，我就22岁了，突然就不想过生日，不想长大。 躲又躲不掉，那就希望我在22岁，脱贫，脱单，哈哈~ 生日快乐 22岁： 有趣有盼，不负心中热爱！]]></content>
      <categories>
        <category>成长</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[《动手学深度学习》]]></title>
    <url>%2F2020%2F09%2F10%2F%E3%80%8A%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B%2F</url>
    <content type="text"><![CDATA[友链： DeepLearning-500-questions 《动手学深度学习》原书 《动手学深度学习》GitHub链接 《动手学深度学习》(PyTorch版)电子书（将《动手学深度学习》 原书中MXNet代码实现改为PyTorch实现） 《动手学深度学习》(PyTorch版)-和鲸项目（选择pytorch镜像直接运行）]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[联邦学习]]></title>
    <url>%2F2020%2F06%2F16%2F%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[友链： TensorFlow Federated框架： https://github.com/tensorflow/federated FATE框架： https://github.com/FederatedAI/FATE B站浙大CS博士解读联邦学习 联邦学习白皮书_v2.0 定义： 约束条件： 价值： 分类 横向联邦学习： 纵向联邦学习： 联邦迁移学习： 框架： 应用实例 车险定价 信贷风控 销量预测 视觉安防 辅助诊断 隐私保护广告 自动驾驶 联邦学习综述 原文： https://arxiv.org/pdf/1912.04977.pdf 翻译： https://xwzheng.gitbook.io/fl/]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>联邦学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[「文献笔记」Can Machine Learning Model with Static Features be Fooled]]></title>
    <url>%2F2020%2F06%2F14%2F%E3%80%8C%E6%96%87%E7%8C%AE%E7%AC%94%E8%AE%B0%E3%80%8DCan-Machine-Learning-Model-with-Static-Features-be-Fooled%2F</url>
    <content type="text"><![CDATA[Can Machine Learning Model with Static Features be Fooled:an Adversarial Machine Learning Approach - 2019 Abstract 实验目标：为了评估机器学习算法在恶意软件检测中的漏洞，我们提出了五种不同的攻击方案来扰乱恶意应用程序，使分类算法对数据点集的判别函数进行不恰当的拟合，最终产生较高的误分类率。 实验过程：为了区分对抗样本和良性样本，提出了两种反防护机制来对抗攻击。在三个不同的基准测试数据集上进行测试，使用各种分类算法，并使用雅可比矩阵和最先进的数据中毒方法比较。 实验结果：生成的对抗样本很大概率可以躲避检测。]]></content>
      <categories>
        <category>文献笔记</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[欢迎关注]]></title>
    <url>%2F2020%2F06%2F11%2F%E6%AC%A2%E8%BF%8E%E5%85%B3%E6%B3%A8%2F</url>
    <content type="text"><![CDATA[数据结构与算法等博客内容 在 我的CSDN 同步更新，欢迎关注！！！ 你好呀：大部分内容我都放在CSDN博客啦，最近在GitHub更新博客比较少，欢迎关注!GitHub博客源码 ，欢迎 star !]]></content>
  </entry>
  <entry>
    <title><![CDATA[测试框架 Mocha]]></title>
    <url>%2F2020%2F05%2F15%2Fmocha%2F</url>
    <content type="text"><![CDATA[友链： https://www.liaoxuefeng.com/wiki/1022910821149312/1101741181366880 http://www.ruanyifeng.com/blog/2015/12/a-mocha-tutorial-of-examples.html mocha[ˈmɒkə]诞生于2011年，是JavaScript的一种单元测试框架，既可以在浏览器环境下运行，也可以在Node.js环境下运行。 mocha的特点主要有： 既可以测试简单的JavaScript函数，又可以测试异步代码，因为异步是JavaScript的特性之一； 可以自动运行所有测试，也可以只运行特定的测试； 可以支持before、after、beforeEach和afterEach来编写初始化代码。 使用Node.js提供的assert模块进行断言 自动化测试知识点补充： 友链： https://blog.csdn.net/wangmiaoyan/article/details/79026647 https://www.ibm.com/developerworks/cn/web/1404_changwz_jasmine/ https://blog.csdn.net/wangmiaoyan/article/details/79082985 JavaScript异步机制详解 理解 JavaScript 的 async/await describe是Jasmine的全局函数，用于创建一个测试套件，可以理解为一组测试用例的集合。describe函数接受两个参数（一个字符串和一个回调函数）。字符串是这个测试套件的名字或标题，通常描述被测试内容，用之前的比喻来说，describe就是一个故事，字符串就是这个故事的标题。回调函数是实现测试套件的代码块（称为describe块）。 it也是Jasmine的全局函数，用来在describe块中创建一个测试用例（spec），和describe一样，it接受两个参数（一个字符串一个回调函数），字符串参数是测试用例的名字或标题，回调函数是实现测试用例的代码块（称为it块）。 describe和it函数的字符串参数是很重要的。如果描述得当的话，你的测试可以将一个完整的测试场景以自然语言的形式表达出来，形成文档。describe和it都属于JavaScript函数，所以JavaScript的作用域规则也适用于此。用java中学过的全局变量与局部变量的概念来说，我们可以把describe类别为一个class，把it类比于里面的方法，在describe中声明的变量即全局变量，在它里面的所以的it中都可以使用。而it中声明的变量为局部变量，只在此it内部有效。 在Jasmine中有四个全局函数用于安装和拆卸，如下： beforeEach：在每一个测试用例（it块）执行之前都执行一遍beforeEach函数 afterEach：在每一个测试用例（it块）执行之后都执行一遍afterEach 函数 beforeAll：在测试套件（describe块）中所有测试用例执行之前执行一遍beforeAll函数 afterAll：在测试套件（describe块）中所有测试用例执行之后执行一遍afterAll函数 Jasmine中describe块代码与it块代码及拆装与卸载的执行顺序： 首先执行的是：其他，不管是外部describe中的其他还是内部describe块中的其他，总之先将其他全部执行，顺序是从上往下。 第二步：找出it块，以it为中心，从外往内找beforeAll,beforeEach，先执行beforeAll，再执行beforeEach，且一个describe中的beforeAll只执行一遍，只有第一个it块执行前会先执行beforeAll,而其他的it块不会有该步骤；而beforeEach则是每个it块执行前都会先执行beforeEach。 第三步：执行it块中代码； 第四步：以it为中心，从内往外执行afterEach和afterAll，先执行afterEach再执行afterAll，执行顺序与beforeAll及beforeEach相反，这里需要注意的是，beforeAll只在该describe块中的最后一个it执行后才会执行，其他it块不会。 Jasmine的断言是期望值与实际值比较，一致则通过，不一致则失败。那么我们比较的类型有多少呢？ 如果两个值是数值，我们可以想到：等于，不等于，大于，小于，大于等于，小于等于，约等于等 如果两个是布尔值，我们可以想到：值为ture，值为false； 如果两个是对象，我们可以想到：对象相等，被定义，未被定义，是否为null 还有一些其他的：根据字符串、正则表达式筛选是否符合规则，是否抛出异常，报错信息是否一致等等，我们还可以自定义匹配器。 同步：如果在函数返回的时候，调用者就能够得到预期结果(即拿到了预期的返回值或者看到了预期的效果)，那么这个函数就是同步的。 异步:如果在函数返回的时候，调用者还不能够得到预期结果，而是需要在将来通过一定的手段得到，那么这个函数就是异步的。 async 是“异步”的简写，而 await 可以认为是 async wait 的简写。所以应该很好理解 async 用于申明一个 function 是异步的，而 await 用于等待一个异步方法执行完成。]]></content>
      <categories>
        <category>测试</category>
      </categories>
      <tags>
        <tag>mocha</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PyTorch学习笔记]]></title>
    <url>%2F2020%2F05%2F12%2Fpytorch%2F</url>
    <content type="text"><![CDATA[友链： PyTorch 1.2中文文档：https://pytorch.apachecn.org/docs/1.2/ 1月16日，Facebook发布了PyTorch 1.4。本次更新的重点是增加了很多重要的新特性，包括给用户提供 Build 级别的移动端定制化支持、增加分布式模型并行训练、让 Java 程序能够运行 TorchScript 等。 此外还有 JIT、C++、分布式训练、Eager 前端、PyTorch Mobile 等方面的功能改进和 Bug 修复。 值得注意的是，本次 PyTorch 更新是最后一个支持 Python2 的版本，同时也是最后一个支持 C++11 的版本。官方提示说，用户应当开始迁移到 Python3，并使用 C++14 开始编译工作。 pytorch安装参考： https://blog.csdn.net/qq_19334535/article/details/104615980 进入pytorch官网：https://pytorch.org/ 点击Get Started 选择正确的package、language、cuda等，执行对应的命令行。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[什么是 A/B 测试？]]></title>
    <url>%2F2020%2F05%2F12%2F%E4%BB%80%E4%B9%88%E6%98%AF-A-B-%E6%B5%8B%E8%AF%95%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[友链： https://www.zhihu.com/question/20045543/answer/1103961403 知乎上鹅厂数据分析师关于 A/B test 的分享。]]></content>
      <categories>
        <category>测试</category>
      </categories>
      <tags>
        <tag>A/B测试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[「文献笔记」Decompiled APK based malicious code classification]]></title>
    <url>%2F2020%2F05%2F05%2F%E3%80%8C%E6%96%87%E7%8C%AE%E7%AC%94%E8%AE%B0%E3%80%8DDecompiled-APK-based-malicious-code-classification%2F</url>
    <content type="text"><![CDATA[April 2020-Decompiled APK based malicious code classification Abstract由于Android恶意软件种类的不断增加，区分每种恶意软件的类型是很重要的。在本文中，我们介绍了一个反编译源代码用于恶意代码分类。这个反编译源代码提供了更深入的分析机会和了解恶意软件的性质。恶意代码与文本的区别在于编译器的语法规则和攻击者逃避潜在检测的努力。因此，我们采用在一定约束下的自然语言处理技术对恶意代码进行分类。首先，本文提出的方法对Android包文件进行分解，然后从源代码中提取API调用、关键字和非混淆的token，并将其分为stop-token、feature-token和long-tail-token。我们还介绍了使用通用的n-tokens来表示通常不太频繁的token。对我们的方法进行了评估，比较了使用API调用和特性权限作为基线，以及它们的组合，以及使用基于反编译APK的神经网络架构。对真实世界中广泛存在的Android恶意软件数据集进行了严格的评估，包括针对71个家族进行分类的24553个应用程序，以及针对60000个应用程序进行恶意代码检测。我们的方法在两个任务中都优于基线。 1.IntroductionThe main contributions of this paper are: 采用反编译源代码分析，包括其中的token，据我们所知，这是首次用于检测和分类恶意代码。 ROCKY-一种用于源代码分析的新方法，它可以创建通用的N-tokens，包括stop(frequent)和long-tail(rare)token。 一个严格的评估，包括将提出的基于反编译源代码的分析与静态分析方法进行比较，使用Android权限、API调用和基于反编译的神经网络架构作为大型真实世界公共数据集的基线。]]></content>
      <categories>
        <category>文献笔记</category>
      </categories>
      <tags>
        <tag>反编译APK</tag>
        <tag>恶意代码分类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[「面试」松果出行测试实习面试]]></title>
    <url>%2F2020%2F02%2F26%2F%E6%9D%BE%E6%9E%9C%E5%87%BA%E8%A1%8C%E5%AE%9E%E4%B9%A0%E9%9D%A2%E8%AF%95%2F</url>
    <content type="text"><![CDATA[一面： 自我介绍 之前实习经历 快排 Java的hashmap vue拦截器 有一个微信登录页面，怎么测试 二面： 自我介绍 springboot和MVC 单链表反转（我是用2个指针反转，面试官问有没有更简单的方法） 二叉树遍历 三次握手 浏览器输入url后的过程 熟悉Linux吗（我说我用Ubuntu系统后面试官没问具体的问题） 你的核心竞争力是什么？长处和短板是什么？怎么克服短板？]]></content>
      <categories>
        <category>面试</category>
      </categories>
      <tags>
        <tag>大四实习面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android混淆-ProGuard]]></title>
    <url>%2F2020%2F01%2F21%2F%E6%B7%B7%E6%B7%86%E6%8A%80%E6%9C%AF%2F</url>
    <content type="text"><![CDATA[参考：https://www.cnblogs.com/duanxz/p/3651418.html 通常情况下，编译后的字节码仍然包含了大量的调试信息：源文件名，行号，字段名，方法名，参数名，变量名等等。这些信息使得它很容易被反编译和通过逆向工程获得完整的程序。有时，这是令人厌恶的。例如像ProGuard这样的混淆器就能删除这些调试信息，并用无意义的字符序列来替换所有名字，使得它很难进行逆向工程，它进一步免费的精简代码。除了异常堆栈信息所需要的类名，方法名和行号外，程序只会保留功能上的等价。 ProGuardProGuard是一款免费的Java类文件压缩器、优化器和混淆器。处理的顺序是先压缩，然后优化，最后才进行混淆。 功能： 压缩(Shrink)：检测并移除代码中无用的类、字段、方法和特性（Attribute）。 优化(Optimize)：对字节码进行优化，移除无用的指令。 混淆(Obfuscate)：使用a，b，c，d这样简短而无意义的名称，对类、字段和方法进行重命名。 预检(Preveirfy)：在Java平台上对处理后的代码进行预检，确保加载的class文件是可执行的。 使用： 下载地址 http://proguard.sourceforge.net/ 解压后，得到的lib包里有proguardgui.jar，是图形界面程序]]></content>
      <categories>
        <category>毕设</category>
      </categories>
      <tags>
        <tag>混淆</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[「文献笔记」首次成功用CNN自动生成代码：北大研究者搞定了炉石传说]]></title>
    <url>%2F2020%2F01%2F02%2F%E3%80%8C%E6%96%87%E7%8C%AE%E7%AC%94%E8%AE%B0%E3%80%8DCNN%E4%BB%A3%E7%A0%81%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%2F</url>
    <content type="text"><![CDATA[2019-北京大学信息科学技术学院-A Grammar-Based Structural CNN Decoder for Code Generation Abstract 代码生成可以将一份程序描述映射为用一种编程语言写成的可执行源代码。现有的方法主要依赖于循环神经网络（RNN）作为解码器。然而，我们发现程序比自然语言句子包含的 token 要多得多，因此 RNN 可能并不适合捕捉长句。本文提出了一个基于语法的结构化卷积神经网络（CNN），用于代码生成。我们的模型通过预测编程语言的语法规则来生成程序；我们设计了若干个 CNN 模块，包括基于树的卷积和前序卷积，其信息通过专用的注意力池化层进一步聚集。在炉石传说基准数据集上的实验结果显示，我们的 CNN 代码生成器的表现超出之前最佳方法 5 个百分点；我们通过另外几个实验在若干语义分析任务上验证了模型的鲁棒性。为了更好地理解模型的每个部分，我们还进行了深入的控制变量测试。 Introduction 代码的抽象语法树（AST）：init(a)：其中的n3、n6两个节点需要拥有父-子节点那样密集的关联，但如果该树是前序穿过序列的，彼此间就会比较远。这对Seq2Seq模型而言就比较困难，传统的Seq2Seq神经网络不能直接建模程序结构。 为了解决这个问题，Dong 和 Lapata (2016) 提出了一种沿着程序的抽象语法树生成代码的方法，但这种生成仍然处于 token 级别。近来，更多的研究通过在每一步预测或重写语法规则 (Xiong et al. 2018; Yin and Neubig 2017; Rabinovich, Stern, and Klein 2017) 来生成程序；因此，确保了生成的程序在语法上是正确的。当在这些方法中使用神经网络时，RNN 被用来捕获解码器中预测的自回归。 在深度学习社区，研究人员对使用卷积网络作为解码器越来越感兴趣 (Gehring et al. 2017; Chaturvedi, Pandit, and Garain 2018)，因为它效率高且容易训练。研究者进一步观察发现，程序比自然语言语句大得多，即使是带有长短期记忆 (Hochreiter and Schmidhuber 1997, LSTM) 单元的 RNN 也存在长期依赖问题 (Bengio, Simard, and Frasconi 1994)。而 CNN，却能通过滑动窗口有效地捕获不同区域的特征。 为此，研究者提出了一种基于语法的结构化 CNN 来用于代码生成。他们的模型根据 AST 中的语法结构规则生成代码，例如，If → expr stmt* stmt* 就遵循了他们先前研究 (Xiong et al. 2018) 中的框架。由于子节点序列是通过一个预测步骤生成的，因此与逐个 token 生成相比，它能够实现更紧凑的预测。换句话说，该模型预测语法规则序列，最终形成整个程序。 在他们的方法中，语法规则的预测主要基于三种类型的信息：指定生成程序的源序列，先前预测的语法规则，以及已经生成的部分 AST。在这里，第一个信息是编码器的输入，后两者使得解码器能够自回归，和以前一样，解码器以编码器为条件。 The Proposed Model 模型概览，虚线箭头表示注意力控制器： Evaluation experiment Ⅰ：hearthstone code generation 研究者在已有的基准数据集HearthStone（炉石传说）上进行了实验。炉石传说数据集的示例卡片，（a）输入描述（b）输出程序：任务是Python代码生成（Ling et al. 2016），该数据集的统计：实验结果表明他们提出的基于CNN的代码生成方法远远超越了以前的基于RNN的方法，新模型与此前业内最佳模型的对比，以百分比记。在手动调整后性能大概能增加 2%（Yin and Neubig (2017)）： experiment Ⅱ：semantic parsing 研究者还进行了扩展性的控制变量测试，表明基于语法的结构化CNN相比一般的CNN应用方法更优越，控制变量测试：如表中所示，新模型在准确率和 BLEU 分数方面都优于以前的所有结果。尤其是，新模型在准确率方面显著高于此前的业内最佳模型——在字符串准确率上高出了 5%。对于手动调整的准确率来说，Yin &amp; Neubig（2017）曾报告过大约 2% 的提升。在本文中，北大的研究者也观察到了类似的效果，实现了 30.3% 的 Acc+分数，这证明了新方法的有效性。 研究者进一步在两个语义解析任务上评估了该方法，其中目标程序比炉石传说的更短；他们的方法依然得到了和以前的最佳方法相当的性能，表明该方法具备鲁棒性，语义分析的准确性（以百分比记）：]]></content>
      <categories>
        <category>文献笔记</category>
      </categories>
      <tags>
        <tag>代码自动生成</tag>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[「文献笔记」maskGAN]]></title>
    <url>%2F2019%2F12%2F29%2F%E3%80%8C%E6%96%87%E7%8C%AE%E7%AC%94%E8%AE%B0%E3%80%8DmaskGAN%2F</url>
    <content type="text"><![CDATA[2018-MASKGAN: better text generative via filling in the ___ Abstract 神经网络文本生成模型通常为自回归（autoregressive）语言模型或者seq2seq模型。这些模型通过序列抽样词语生成文本（抽样分布将前一个词语作为条件），并且对于几种机器翻译和摘要总结的基准是较先进的模型。这些基准通常通过验证复杂度定义，尽管这不是直接对生成文本质量的衡量。除此以外，这些模型通常使用极大似然或者 teacher forcing进行训练。这些方法非常适合优化复杂度（perplexity），但是可能导致样本质量差，因为生成文本需要将可能从来没有在训练过程中观察到的词语序列作为条件。我们提出使用生成对抗网络（GAN）来提高样本质量，它通过显式地训练生成器产生高质量样本，并且已经在图像生成领域取得很大成功。GAN 最初设计用于输出可微分的值，所以生成离散语言是具有挑战性的。我们认为验证复杂度本身不代表模型生成的文本质量。我们引入条件actor-critic GAN，它可以基于上下文填充缺失的文本。我们从定性和定量的角度证明，相比较大似然训练的模型，这种方法生成了更真实的有条件和无条件文本样本。 MaskGAN notation architecture： training alternative approaches for long sequences and large vocabularies method details]]></content>
      <categories>
        <category>文献笔记</category>
      </categories>
      <tags>
        <tag>GAN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[「文献笔记」DCGAN]]></title>
    <url>%2F2019%2F12%2F27%2F%E3%80%8C%E6%96%87%E7%8C%AE%E7%AC%94%E8%AE%B0%E3%80%8DDCGAN%2F</url>
    <content type="text"><![CDATA[2016-基于深度卷积对抗生成网络的无监督表示学习 unsupervised representation learning with deep convolutional generative adversarial networks Abstract 近年来，CNNs的监督学习在计算机视觉应用中得到了广泛的应用。相对而言，使用CNNs进行无监督学习的研究较少。在这项工作中，我们希望能够帮助缩小CNNs在监督学习和非监督学习方面的成就的差距。我们介绍了一类称为深度卷积生成对抗网络(DCGANs)的CNNs，它具有一定的架构约束，并证明了它们是无监督学习的有力候选者。通过对各种图像数据集的训练，我们给出了令人信服的证据，证明我们的深卷积对偶在生成器和鉴别器中都学习了从对象部件到场景的表示层次。此外，我们将学习到的特性用于新任务——演示它们在一般图像上的适用性。 Introduction 我们提出了一种卷积GANs的架构，它能在大多数设置下稳定训练。我们把它命名为深度卷积GANs。 我们在图像分类任务上使用了预训练的分类器，来展示和其他无监督算法的竞争性。 我们可视化了GANs学习到的滤波器，并通过经验证明了特定的滤波器已经学会了绘制特定的图像。 我们证明了生成器具有有趣的向量算术属性，可以方便地操作生成的示例的许多语义质量。 Related Work 无标签数据的表征学习：一个经典方法是对数据进行聚类(例如使用K-means)，并利用聚类来改进分类分数。在图像领域中，可以对图像块学习强大的图像表示。另一个流行的方法是训练自动编码器,编码一个图像到一个压缩码,编码和解码使重建图像尽可能准确。深度信念网络在学习层次表示方面也表现得很好。 生成自然图像：生成图像模型得到了广泛的研究，分为参数化和非参数化两大类。 可视化CNNs的内部结构：在CNNs的背景下，Zeiler等人通过反卷积和最大池化，可以找到网络中每个卷积滤波器的近似目的。类似地，对输入使用梯度下降让我们检查激活某些过滤器子集的理想图像(Mordvintsev等)。 Approach and Model Architecture 我们的方法的核心是采用和修改最近演示的对CNN架构的三个更改。 第一个是全卷积网络，它用步长卷积替换了确定性的空间池化函数(如maxpooling)，允许网络学习自己的空间下采样。我们在生成器中使用这种方法，允许它学习自己的空间上采样和鉴别器。 第二个是在卷积特征之上消除完全连接层的趋势。这方面最有力的例子是全局平均池化，它已被用于最先进的图像分类模型(Mordvintsev等)。我们发现，全局平均池化提高了模型的稳定性，但不利于收敛速度。将最高卷积特性直接连接到生成器和鉴别器的输入和输出的中间地带工作得很好。GAN的第一层以均匀的噪声分布Z作为输入，由于只是矩阵乘法，可以称为全连通，但结果被重新构造为一个四维张量，并作为卷积堆栈的起点。对于鉴别器，最后一个卷积层被平坦化，然后输入一个单一的sigmoid输出。 第三个是批归一化，通过将每个单元的输入归一化为零均值和单位方差来稳定学习。这有助于处理由于初始化不良而出现的训练问题，并有助于在更深层次的模型中实现梯度流。这对于让深层生成器开始学习非常关键，可以防止生成器将所有样本压缩到一个点，这是在GANs中观察到的常见故障模式。然而，直接将BatchNorm应用于所有层，会导致样品振荡和模型不稳定。通过不对生成器输出层和鉴别器输入层应用BatchNorm，避免了这种情况。除输出层使用Tanh函数外，生成器使用ReLU激活。我们观察到，使用有界激活可以使模型更快地学习饱和和覆盖训练分布的颜色空间。在鉴别器中，我们发现LeakyReLU激活效果很好，特别是对于高分辨率的建模。这与最初使用maxout激活的GAN论文形成了对比。 稳定的DCGAN结构指南 用卷积和反卷积代替所有池化层，G网络中使用转置卷积（transposed convolutional layer）进行上采样，D网络中用加入stride的卷积代替pooling 在D和G中均使用批归一化batch normalization 为了更深的网络，移除了全连接隐层 在生成器中使用ReLU激活函数（除了在输出的时候用了Tanh激活函数） 在判别器中使用了LeakyReLU激活函数 Details of Adversarial Training 除了缩放到tanh激活函数的范围[-1,1]外，训练图像没有进行其他预处理。所有模型均采用mini-batch SGD进行训练，batch为128。所有的权值都是从0为中心的正态分布初始化的，标准差为0.02。在LeakyReLU中，所有模型的泄漏斜率都设置为0.2。以前的GAN工作使用动量来加速培训，而我们使用Adam优化器(Kingma &amp; Ba, 2014)调优超参数。我们发现建议的学习率为0.001，用0.0002代替，太高了。此外,我们发现离开动量项β1的建议值为0.9时导致培训振荡和不稳定,同时减少到0.5帮助稳定训练。 除了缩放到tanh激活函数的范围[-1,1]外，训练图像没有进行其他预处理。所有模型均采用mini-batch SGD进行训练，batch为128。所有的权值都是从0为中心的正态分布初始化的，标准差为0.02。在LeakyReLU中，所有模型的泄漏斜率都设置为0.2。以前的GAN工作使用动量来加速培训，而我们使用Adam优化器(Kingma &amp; Ba, 2014)调优超参数。我们发现建议的学习率为0.001，用0.0002代替，太高了。此外,我们发现离开动量项β1的建议值为0.9时导致培训振荡和不稳定,同时减少到0.5帮助稳定训练。 在下面三个数据集训练DCGAN LSUN （大规模场景理解） Faces Imagenet-1K Empirical Validation of DCGANs Capabilities classifying CIFAR-10 using GANs as a feature extractor（使用GANS作为特征提取器对CIFAR-10进行分类） classifying SVHN digits using GANs as a feature extractor（使用GANS作为特征提取器对SVHN数字进行分类） Investigating and Visualizing the Internals of the Networks walking in the latent space（漫游隐空间）：各种各样的学习通常能告诉我们记忆的迹象（如果有急剧的转变）以及空间分层坍塌的方式。如果在这个潜在的空间中行走导致图像生成语义的变化（例如被添加和删除的对象），我们可以推断该模型已经学习了相关和有趣的表示。 visualizing the discriminator features（可视化判别器特性）：以前的工作已经证明，在大型图像数据集上进行有监督的CNN训练会产生非常强大的学习功能。此外，在场景分类方面受监督的CNN学习对象检测。我们证明了在大图像数据集上训练无监督的DCGAN也可以学习有趣的特征层次。相比之下，在同一图中，我们为随机初始化特征给出了基线，这些特征在语义上相关或有趣的任何事物上不被激活。 manipulating the generator representation（操纵生成器表示） forgetting to draw certain objects（忘记画特定的对象） vector arithmetic on face samples（向量算法的人脸样本）]]></content>
      <categories>
        <category>文献笔记</category>
      </categories>
      <tags>
        <tag>GAN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac配置]]></title>
    <url>%2F2019%2F12%2F27%2FMac%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[安装Homebrew链接：https://brew.sh/index_zh-cn.html 安装命令： 1/usr/bin/ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot; 安装过程中出现的问题：报错curl: (7) Failed to connect to raw.githubusercontent.com port 443: Operation 解决方法：https://blog.csdn.net/zbc415766331/article/details/104128351/ 下载文件brew_install.rb，控制台运行ruby brew_install.rb命令。此文件下载地址： https://pan.baidu.com/s/1rVh8bY73NLc77cQYN_2HoQ 密码：n3si 安装wget安装wget： 1brew install wget wget访问https出现问题wget https://www.apache.org/ ——出现unsupported scheme提示，这是必然的，因为这是个安全链接。 加个选项试试，不检查证书，wget –no-check-certificate https://www.apache.org/ ，提示没有这个选项 wget: unrecognized option –no-check-certificate 解决方法：参考：https://www.jianshu.com/p/94bb06811a26 查找wget: 1find /usr/ -name &quot;wget&quot; 输出： 123/usr//local/bin/wget/usr//local/Cellar/wget/usr//local/Cellar/wget/1.19.5/bin/wget 删除系统默认版本： 1sudo rm -f /usr/local/bin/wget 解决: 1sudo cp /usr/local/Cellar/wget/1.19.5/bin/wget /usr/local/bin/ 安装anaconda3下载地址：https://www.anaconda.com/distribution/#macos 下载哪一个都可以，一个是窗口安装，一个是命令行安装，以命令行为主： cd到下载目录下，执行如下代码： 1bash Anaconda3-5.3.0-MacOSX-x86_64.sh 添加环境变量： 1sudo vim ~/.bash_profile 在 .bash_profile 文件中添加下面文本: 1export PATH=&quot;/Users/xxx/anaconda3/bin:$PATH&quot; 刷新生效source: 1source ~/.bash_profile 卸载Anaconda123rm -rf ~/anaconda3vim ~/.bash_profilerm -rf ~/.condarc ~/.conda ~/.continuum 手动删除文件夹，然后再去把配置文件里面对应的环境变量删了。 Fliqlo翻页时钟参考：https://zhuanlan.zhihu.com/p/105069716?utm_source=wechat_session 安装iTerm2不得不说mac自带的终端是真的丑，来配置下iTerm2吧 参考：https://zhuanlan.zhihu.com/p/37195261 安装Oh my zsh过程中出现的问题：raw.githubusercontent.com (raw.githubusercontent.com)|::|:443… 失败：拒绝连接 解决方法：https://blog.csdn.net/wowbing2/article/details/105797442/ 进入网站： https://site.ip138.com/raw.Githubusercontent.com/ 输入 raw.githubusercontent.com ，查询其相关的IP地址 终端输入 sudo vi /etc/hosts 添加 151.101.xx.133 raw.githubusercontent.com 保存，退出 :wq]]></content>
      <categories>
        <category>Mac</category>
      </categories>
      <tags>
        <tag>Mac</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[「文献笔记」GANs]]></title>
    <url>%2F2019%2F12%2F27%2FGAN%2F</url>
    <content type="text"><![CDATA[2014-GANs开山之作 Generative Adversarial Networks - github源码 Abstract 论文基于对抗过程提出了估计生成模型的新框架，主要的做法就是：同时训练2个网络：G网络和D网络，G网络主要是通过学习获取信息分布，使用隐空间的随机变量生成接近于真实的数据，即使得D网络将生成的数据识别为真实的训练数据；而D网络主要是通过学习，尽可能地区分开真实的训练数据与生成的虚假数据。G网络和D网络都是可微的，因此可以使用BP进行学习。 Introduction 之前在判别式模型方面已经有很多的研究，比如说分类等任务，但是生成模型由于其最大似然估计和相关策略设置都很复杂，因此目前研究较少，本文则是提出了一种新的生成模型，避免了之前的这些问题。 提出的对抗网络中，D网络是通过学习，判断一个样本是由模型分布（fake）还是有数据分布（real）生成的。G网络则是生成fake samples，使得D网络尽可能混淆真实与虚假数据。 论文提出的对抗网络不需要使用markov链，只需要使用BP即可，再结合dropout等tricks。 Related Work 在之前，大部分在deep generative model上的工作都是希望能够得到一个参数化的概率分布，可以通过最大似然估计进行学习，比较成功的模型有deep Boltzmann machine等。 之前比较类似的工作是VAE，即使用encoder将训练数据映射到高斯分布中，再使用decoder将其变换到原始的训练数据，从而实现对数据的参数化分布表示。 Adversarial Nets 定义噪声变量pz(z)，表示generator的分布，可以通过映射关系，将噪声映射到对应的data space，这个映射可以表示为G(z,θg)，MLP的参数为θg，G是可微的（可学习的）。同时定义D(x,θd)，输出一个值，D(x)表示x来自真实数据而非pg的概率。因此主要的工作就是训练D，尽可能使得训练样本和G生成的样品都被赋予正确的label。最终的优化目标是：下面这幅图片很好地描述了这个过程： 在这里需要注意的是，因为需要最大化D，因此我们使用的是梯度上升法。最终pdata(x)=pg(x)，D(x)=12，在训练的过程中，G和data的分布变化如下 Theoretical Results Global Optimality of pg=pdata 如果G和D都有足够的容量，则在每次迭代过程中，D都会到达其最优解，之后在更新G的时候，G都会进行优化，使得pg向pdata收敛 Experiments 作者在mnist、TFD、cifar-10数据集上进行了实验。对生成的结果基于Parzen window的似然估计进行评估，在mnist上，相对之前的DBN、stacked CAE等方法，效果更好一些。 Advantages and Disadvantages disadvantages：没有获得对pg(x)的显性表示，D也必须和G同时训练，共同优化参数，如果只训练其中的一个， 无法获得很好的效果。 advantages：不再需要markov chains，在训练的时候，直接使用BP即可，并且可以很容易地在模型中嵌入很多其他的函数或者变换。 Conclusion and Future Work CGAN，即根据给定的条件和随机分布，生成特定的数据。 通过训练一个给定x，预测z的辅助网络，用于样本之间的相似度检测。 可以训练一个shared model，给定任意子条件和随机分布，生成该条件对应的样本。 半监督学习：当训练数据有限时，可以使用discriminator的特征或者G网络来提升分类器的性能。 在训练的过程中，如果可以确定一个更好的z的分布，则训练速度和模型性能都会大大提升。 在训练的过程中，如果可以确定一个更好的z的分布，则训练速度和模型性能都会大大提升。 一张图总结一下生成模型以及本文的对抗模型，证明了GAN的光明前景。 参考资料： github地址 相关博客讲解 论文解读 DeepLearning-500-questions/ch07_生成对抗网络(GAN) 训练 GANs 一年我学到的 10 个教训 GAN整整6年了！是时候要来捋捋了！ GAN 真的创造了新的信息吗？ GAN万字长文综述：https://zhuanlan.zhihu.com/p/58812258 GAN六年整理：https://zhuanlan.zhihu.com/p/94206978 纳什均衡]]></content>
      <categories>
        <category>文献笔记</category>
      </categories>
      <tags>
        <tag>GAN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Deep Learning时代最好用的云GPU——Google Colab]]></title>
    <url>%2F2019%2F12%2F22%2FDeep-Learning%E6%97%B6%E4%BB%A3%E6%9C%80%E5%A5%BD%E7%94%A8%E7%9A%84%E4%BA%91GPU%E2%80%94%E2%80%94Google-Colab%2F</url>
    <content type="text"><![CDATA[Colab是Google基于Google Drive存储的对外免费开放的云服务器，主要有CPU,GPU,TPU三种可选硬件加速方案。最近，Colab 将 以前的 K80 替换为 Tesla T4，新一代图灵架构、16GB 显存，重点是免费 GPU！免费！免费！。因此强烈建议大家赶紧薅谷歌的羊毛，获取强大的免费算力。 原文链接：https://blog.csdn.net/jinyuan7708/article/details/89948938 Googole Colab官网链接：https://colab.research.google.com/notebooks/welcome.ipynb#recent=true 使用Google Colab运行或导入.py文件 12from google.colab import drivedrive.mount(&apos;/content/drive/&apos;) 单击出现的链接，复制验证码并将其粘贴到文本框中。完成授权过程后，就可以了。 通过以下方式与Google联系 1!ls &quot;/content/drive/My Drive/&quot; 在Colab中cd命令是无效的，切换工作目录使用chdir函数 12import osos.chdir(&quot;drive&quot;) 回到上级目录 12import osos.chdir(&quot;../&quot;) 如果要将.csv文件从url下载到“Colab tutorial”文件夹，只需运行 1! wget https://raw.githubusercontent.com/vincentarelbundock/Rdatasets/master/csv/datasets/Titanic.csv -P&quot;/content/drive/My Drive/Colab tutorial&quot; 查看您当前是否在Colab中使用GPU 12import tensorflow as tftf.test.gpu_device_name() 查看使用的是哪种GPU 12from tensorflow.python.client import device_libdevice_lib.list_local_devices() RAM怎么样？ 1! cat /proc/meminfo 那CPU怎么样？ 1! cat /proc/cpuinfo 查看tensorflow版本号 12import tensorflow as tftf.__version__ 安装tensorflow 1.8 1!pip install tensorflow==1.8.0rc]]></content>
      <categories>
        <category>colab</category>
      </categories>
      <tags>
        <tag>colab</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[「面试」字节实习一面凉经]]></title>
    <url>%2F2019%2F11%2F25%2F%E5%AD%97%E8%8A%82%E5%89%8D%E7%AB%AF%E5%AE%9E%E4%B9%A0%E9%9D%A2%E8%AF%95%2F</url>
    <content type="text"><![CDATA[这篇博客没有任何学习意义，因为问的我基本都不会，后面也没有二面三面了，只用来记录一次悲惨的面试。学习面试经验的小伙伴不用点开看了 前端实习大致问的问题：（因为我是真不会，只能大致记起来一丢丢） javascript基本数据类型，给了一段代码让我说运行结果并进行分析，作用域等等 我简历上写了vue，就问了一点vue的问题，没错我也没有回答上来 git基本命令。面试官问：正在一个分支上写东西，突然需要修改另一个分支，怎么做。 主要写一下两个算法题吧，面试官给了题目要求，让我大致写出来代码，不需要运行，然后解释大概思路。面试官说因为是前端面试，最好用javascript写，但是javascript真心不熟，最后用的c++。 顺时针打印二维数组我当时的思路是这样的：数据结构用vector，m,n分别是数组的行和列数，k记录循环圈数，初始值为0，i,j记录正在打印的元素下标 12345678while(k=min(m/2,n/2)&#123; while(j&lt;n-k)cout&lt;&lt;a[i][j++] //右 while(i&lt;m-k)cout&lt;&lt;a[i++][j] //下 while(j&gt;=n+k)cout&lt;&lt;a[i][j--] //左 while(i&gt;=m+k)cout&lt;&lt;a[i--][j] //上 k++ //循环完一圈&#125; 给出一个字符数组，输出出现次数最多的三个字符，如果出现次数一样，输出先出现的字符我回答的就是用map统计每个数组出现的次数，对次数进行排序（后面想想这个思路是有问题的，还好当时面试官没有深入问下去，或者面试官已经看出来思路有问题但是没戳破～～～） 测开实习 自我介绍 百度实习经历：因为在百度也是做的测试吧，这部分问的比较多。问了，版本迭代过程，bug生命周期，在百度做测试实习学到了什么，自动化case用的什么语言，自动化case编写方法等等。 一个学院网站制作的项目：前端+后端，输入数据后整个传输过程。 发的一篇论文：论文主要是做的xx算法改进，问了下改进之前的准确率和我的论文里的准确率，用的什么方法提高准确率。 数据库：一个表name，其中一列是a，a里面有张三李四这些名字，要求找出出现次数大于三次的名字。 算法：找出一个整数数组的最大值。二叉树的后续遍历。 最后是老问题：你有什么想问我的吗。问了面试官部门自动化测试和无技术测试比例，面试官说一半一半吧，各占50%。 版本迭代： [迭代立项] 项目经理&amp;产品进行需求收集，进行迭代立项，确定迭代主要重点（如：此期版本迭代重点是UI改版、功能优化、正常版本迭代、框架重构……） [产品需求分析] 产品：产品进行需求规划分析，详细需求设计输出； 开发经理：开发经理进行整体需求时间及风险预估，然后将大致需求分配到各个开发人员； 开发同学：对一些已经输出的需求，可以提前熟悉需求，做需求时间及风险预估； 测试经理：测试经理进行整体需求时间及风险预估，然后将需求分配到各个测试人员； 测试同学：测试同学进行初步需求分析，对已经明确输出的需求，可以提前介入编写测试用例； [需求评审] 产品：进行详细的需求宣讲，开发、测试时间预估； 开发：设计到相关需求的开发进行需求评审，给出开发时间预估； 测试：设计到相关需求的测试进行需求评审，给出测试用例及模块测试时间预估； [评审完成] 产品：根据开发及测试给出的模块开发测试及集成测试时间，安排上线时间； 开发：编写各个需求模块的测试用例，标记号p0级别测试用例； 设计：涉及到需要设计介入的需求，设计进行UI及交互输出，然后给到开发； [开发中] 测试同学：进行用例评审，开发提测必须P0用例通过；如遇提测的模块需求，开展模块测试。 开发同学：完成模块开发，执行P0用例，自测通过后，提交模块测试； 产品经理&amp;测试经理&amp;项目经理：跟进集成进度，评估集成风险； [测试中] 测试同学：进行模块测试，模块测试完成后，执行集成、性能测试； 测试经理：跟进测试进度，评估测试风险； [灰度发布] 测试：集成测试完成，评估质量风险可控，进行灰度发布； 产品：进行上线前体验及产品功能验收； 运营：灰度发布，反馈收集，数据统计； [正式上线] 运营：上线发布，用户收集，线上运营； 测试：线上功能验证、迭代测试报告输出、性能测试输出； 产品：进行产品发布说明，迭代总结； 链接：https://www.jianshu.com/p/4fa9fcea5ecc bug生命周期：发现bug–&gt;提交bug–&gt;指派bug–&gt;研发确认bug–&gt;研发修复bug–&gt;回归验证bug–&gt;是否通过验证–&gt;关闭bug。]]></content>
      <categories>
        <category>面试</category>
      </categories>
      <tags>
        <tag>大四实习面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[「面试」百度测开实习]]></title>
    <url>%2F2019%2F11%2F15%2F%E7%99%BE%E5%BA%A6%E6%B5%8B%E5%BC%80%E5%AE%9E%E4%B9%A0%E9%9D%A2%E8%AF%95%2F</url>
    <content type="text"><![CDATA[一面主管面一面大约20多分钟. 1、自我介绍 2、python （1）字典： （2）字典怎么获取所有键： （3）列表和元组的区别： 列表可以看成是动态数组,它们是可变的并且可以重新设定长度 元组可以看成是静态的数组,它们是不可变的,并且长度也是一旦创建就无法改变 从设计上来说: 列表是用来保存多个相互独立对象的数据集合 元组设计的初衷就是为了描述一个不会改变的事物的多个属性 （4）list排序：list.sort(cmp=None, key=None, reverse=False) （5）浅拷贝与深拷贝的区别： Python3中，有6个标准的数据类型，他们又分为可变和不可变： 不可变数据（3个）：Number（数字）String（字符串）Tuple（元组）。 可变数据（3个）：List（列表）Dictionary（字典）Set（集合） 浅拷贝 对于不可变类型Number String Tuple,浅复制仅仅是地址指向，不会开辟新空间。 对于可变类型 List、Dictionary、Set，浅复制会开辟新的空间地址(仅仅是最顶层开辟了新的空间，里层的元素地址还是一样的)，进行浅拷贝 浅拷贝后，改变原始对象中为可变类型的元素的值，会同时影响拷贝对象的；改变原始对象中为不可变类型的元素的值，只有原始类型受影响。 （操作拷贝对象对原始对象的也是同理） 深拷贝 浅拷贝除了顶层拷贝，还对子元素也进行了拷贝（本质上递归浅拷贝） 经过深拷贝后，原始对象和拷贝对象所有的子元素地址都是独立的了 可以用分片表达式进行深拷贝 字典的copy方法可以拷贝一个字典 3、你学过数据结构吗？说说二分查找 4、说一下面向对象的特征？ 封装：封装把一个对象的属性私有化，同时提供一些可以被外界访问的属性的方法，如果属性不想被外界访问，我们大可不必提供方法给外界访问。但是如果一个类没有提供给外界访问的方法，那么这个类也没有什么意义了。 继承：继承是使用已存在的类的定义作为基础建立新类的技术，新类的定义可以增加新的数据或新的功能，也可以用父类的功能，但不能选择性地继承父类。通过使用继承我们能够非常方便地复用以前的代码。 多态：所谓多态就是指程序中定义的引用变量所指向的具体类型和通过该引用变量发出的方法调用在编程时并不确定，而是在程序运行期间才确定，即一个引用变量到底会指向哪个类的实例对象，该引用变量发出的方法调用到底是哪个类中实现的方法，必须在由程序运行期间才能决定。 5、了解微信小程序吗？ 6、了解测试吗？给你一个页面怎么进行测试？比如微信的登录界面。 其他就是问了项目里面的一些东西，比如数据库是怎么设计的等等。 二面主管面二面也是先自我介绍，接下来就是说到一个点面试官比较感兴趣，就一直深入提问。 了解开发测试吗？ 了解小程序吗？ 除了微信小程序其他的呢，了解百度的小程序吗？ 说一下在做项目的过程中遇到的一个问题，怎么解决的，具体的说一下细节。等等等。 最后问：你有什么问题问我吗？ 三面经理面跟二面类似，先自我介绍，我是自己搭了这个博客，把链接附到简历上，整个面试三十多分钟面试官一直在问关于这个博客的问题。 博客搭好后怎么测试的？ 说一下在做项目的过程中遇到的一个问题，怎么解决的，具体的说一下细节。 你看过哪些关于技术方面的书？ 最后问：你有什么问题问我吗？ 2020.8 更： 把实习的经历做成了一个视频： https://www.bilibili.com/video/BV1oa4y1L7aJ 欢迎三连哦！]]></content>
      <categories>
        <category>面试</category>
      </categories>
      <tags>
        <tag>大四实习面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android恶意代码检测：N-gram Opcode]]></title>
    <url>%2F2019%2F11%2F10%2FN-gram%2F</url>
    <content type="text"><![CDATA[读硕士论文的时候发现很多检测Android恶意代码用到了N-gram，总结一下这方面的学习资料。 参考资料： 用机器学习检测Android恶意代码 – runner 博客原文 博客实现的代码 kaggle malware-classification kaggle_Microsoft_Malware github java、class、dex、smali、jar、apk关系 N-gramn-gram是自然语言处理领域的概念，早期的语音识别技术和统计语言模型与它密不可分。n-gram基于一个简单的假设，即认为一个词出现的概率仅与它之前的n-1个词有关，这个概率可从大量语料中统计得到。例如“吃”的后面出现“苹果”或“披萨”的概率就会比“公路”的概率大(正常的语料中基本不会出现“吃公路”这种组合)，可以看出n-gram在一定程度上包含了部分语言特征。 将n-gram应用于恶意代码识别的想法最早由Tony等人在2004年的论文N-gram-based Detection of New Malicious Code 中提出，不过他们的方法是基于ByteCode的。2008年Moskovitch等人的论文Unknown Malcode Detection Using OPCODE Representation 中提出利用OpCode代替ByteCode更加科学。 用机器学习检测Android恶意代码 – runner 安卓恶意代码检测方法 基于特征代码的检测方法，通过检测文件是否拥有已知恶意软件的特征代码(如一段特殊代码或字符串)来判断其是否为恶意软件。它的优点是快速、准确率高、误报率低，但是无法检测未知的恶意代码。 基于行为的分析方法又分为动态分析方法和静态分析方法，依靠监视程序的行为与已知的恶意行为模式进行匹配，以此判断目标文件是否具备恶意特征。它的优点可以检测未知的恶意代码变种，缺点是误报率较高。 动态分析方法是指利用“沙盒或模拟器”来模拟运行程序，通过监控或者拦截的方式分析程序运行的行为，但是很消耗资源和时间。 静态分析方法则是通过逆向手段抽取程序的特征，分析其中指令序列等。本文采用静态分析的方法进恶意行代码检测。 Weka与机器学习的分类算法Weka（Waikato Environment for Knowledge Analysis），是一款免费的，非商业化基于JAVA环境下开源的机器学习（machine learning）以及数据挖掘（data minining）软件。Weka存储数据的格式是ARFF（Attribute-Relation File Format）文件，是一种ASCII文本文件。本文就是将特征数据生成ARFF格式的文件，利用Weka自带的分类算法进行数据训练与模型测试。机器学习中分为有监督学习与无监督学习。有监督学习就是根据训练集，用学习算法学习出一个模型，然后可以用测试集对模型进行评估准确度和性能。分类算法属于有监督学习，需要先建立模型。常见的分类算法有：随机森林(Random Forest)、支持向量机(SVM)等。 APK的基本格式APK文件格式是一种基于ZIP的格式，它与JAR文件的构造方式相似。参考：https://en.wikipedia.org/wiki/Android_application_package一个APK文件通常包含以下文件： classes.dex: Dalvik字节码,可被Dalvik虚拟机执行。（需要重点注意，安卓的执行代码被编译后封装在这个文件中。） AndroidManifest.xml: 一个的Android清单文件，用于描述该应用程序的名字、版本号、所需权限、注册的服务、链接的其他应用程序。该文件使用XML文件格式。 META-INF 文件夹: 下面有3个文件 MANIFEST.MF: 清单信息 CERT.RSA: 保存应用程序的证书和授权信息 CERT.SF: 保存SHA-1信息资源列表 res: APK所需要的资源文件夹 assets: 不需编译的原始资源文件目录 resources.arsc:编译后的二进制资源文件 lib:库文件目录 Dalvik虚拟机与反汇编区别与JAVA虚拟机(JVM)，安卓的虚拟机称为Dalvik虚拟机（DVM）。Java虚拟机运行的是Java字节码，Dalvik虚拟机运行的是Dalvik字节码。Java虚拟机基于栈架构，Dalvik虚拟机基于寄存器架构。DVM拥有专属的DEX可执行文件格式和指令集代码。smali和baksmali 则是针对DEX执行文件格式的汇编器和反汇编器，反汇编后DEX 文件会产生.smali 后缀的代码文件，smali代码拥有特定的格式与语法，smali语言是对Dalvik 虚拟机字节码的一种解释。 恶意代码模型测试恶意代码检测模型 基于机器学习的安卓恶意应用检测方法研究 N-gram Opcode特征实际上是dalvik Opcode特征结合自然语言处理中的N-gram方法而产生的特征集合。 检测框架： N-gram Opcode： github项目恶意代码检测过程通常可以分为三个步骤： 特征提取与选择 选取适当的分类模型 获取分类结果其中关键技术就是恶意代码特征提取，特征的质量直接影响恶意代码的检测效果，目前提出的基于机器学习的恶意软件分类方法在恶意软件特征提取方法上存在较大差异。 malware_classification_system这个项目没有readme，不知道怎么跑，里面还有html、css代码，还用到了flask_bootstrap包，看起来更像一个网站。 MalwareClassification（这个代码看上去不错，跑一下试试）gramfeature.csv文件存储特征，分为训练数据和测试数据VirusShare.csv和yingyongbao.csv存储数据id和classtpr_fpr.py文件为模型检测程序.txt文件为ROC曲线实验数据plot.py为ROC曲线画图程序其中，良性样本选自yingyongbao（yingyongbao.csv：class为０），恶性样本选自virusshare（VirusShare.csv：class为１）。还用到了TF-IDF（term frequency–inverse document frequency），一种用于信息检索与数据挖掘的常用加权技术。TF意思是词频(Term Frequency)，IDF意思是逆文本频率指数(Inverse Document Frequency)。 MalwareAnalyze需要Linux环境,需要安装IDA PRO反汇编软件,需要viper环境 https://github.com/viper-framework/viper.config.py 环境配置文件pe_select.py 提取样本中的PE文件idabatch.pu ida反汇编批处理feature.py 特征处理sequence3.py 提取特征序列vector_batch.py &amp;&amp; vector_batch2.py 向量化批处理vectoring.py 向量化train.py 训练集 用于viper平台集成功能test.py 测试集 用于viper平台集成功能ml.py 训练模型 用于viper平台集成功能 ToyMalwareClassificationKaggle微软恶意代码分类比赛说明和数据下载 https://www.kaggle.com/c/malware-classification/代码说明randomsubset.py 抽取训练子集asmimage.py ASM文件图像纹理特征opcode_n-gram.py Opcode n-gram特征firstrandomforest.py 基于ASM文件图像纹理特征的随机森林secondrandomforest.py 基于Opcode n-gram特征特征的随机森林combine.py 将两种类型的特征结合运行说明将完整的训练数据集解压，修改randomsubset.py中的路径并运行修改asmimage.py和opcode_n-gram.py中的路径，并运行run.sh，耐心等待即可看到结果 Software-system-securitymd用于提交软件与系统安全实验和最终大作业通过修改程序可执行文件的方式（不是修改源代码），使得程序运行后显示的内容不为hello world，变成 hello cuc！上一题的程序中，修改的显示内容变为一个很长的字符串（至少2kb长）。并且保证程序正常运行不崩溃。在notepad（32位64位均可）中，输入一段文字。然后使用调试器，在内存中修改这段文字。使得没有在界面操作notepad的修改文字的情况下。notepad中显示的文字变化。通过调试器监控计算器程序的运行，每当运行结果为666时，就改为999。通过API hook的方法，在每次notepad保存txt文件时，就将文件内容修改为： “you have been hacked!”通过API hook的方法，使得cmd的dir命令看不到任意目录下的hacker.exe software_security Android基于opcode的N-gram安卓恶意软件检测 下面是我已经运行成功的code，已经push到我的github上了，都在readme中注明了出处。 AndroidMalware-ngram-RF这个代码在原有基础上实现了用机器学习算法实现恶意代码和良性代码的分类。 学习资料：Android Dalvik 指令集smali 文件格式(apktool)]]></content>
      <categories>
        <category>毕设</category>
      </categories>
      <tags>
        <tag>N-gram Opcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CNN实现olivettifaces人脸数据库识别 附源码[转]]]></title>
    <url>%2F2019%2F11%2F07%2FCNN%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%BA%94%E7%94%A8%E4%BA%8E%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%EF%BC%88%E8%AF%A6%E7%BB%86%E6%B5%81%E7%A8%8B-%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%EF%BC%89%2F</url>
    <content type="text"><![CDATA[选修课生物识别技术期末要做一个报告，选择了人脸识别，参考了github的一个源码，整理一下思路。 本文为CSDN博主「wepon_」的原创文章，转载请附上原文出处链接及本声明。原文链接：https://blog.csdn.net/u012162613/article/details/43277187 。 我本地系统是ubantu18.04，anaconda3，直接运行会报错，一些包需要修改成python3对应的包。本地运行需要配置一大堆东西，所以修改后我是在kaggle上运行的，实测有效。完整代码在我的kaggle。 除此之外还有对应的论文报告，有需要的可以私聊我。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>人脸识别</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[个人博客更换成自己的域名]]></title>
    <url>%2F2019%2F11%2F05%2F%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%9B%B4%E6%8D%A2%E6%88%90%E8%87%AA%E5%B7%B1%E7%9A%84%E5%9F%9F%E5%90%8D%2F</url>
    <content type="text"><![CDATA[在腾讯云花了五毛钱买了一个一年的域名，决定把我的博客更换成自己的域名！2020.11.2更：去年买的域名到期啦，又在腾讯云买了一个 新地址是 http://www.dccun.work/ ~ 参考：https://blog.csdn.net/qq_42540989/article/details/95321281 打开管理控制台解析 打开cmd ping 一下你的（例如：xxx.github.io） 就可以得到 这个就是记录值 如下图所示填入解析页面 在Hexo目录下的source中建一个CNAME命名的文本文件 绑定域名进入github 项目中，点击setting，进入setting页面后，往下找，改成你购买的域名 这样就能通过访问我们购买的域名去访问博客 最后要说的是：博客源码 ，欢迎 star !]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于GitHub搭建的个人博客进行备份]]></title>
    <url>%2F2019%2F10%2F30%2F%E4%BD%BF%E7%94%A8hexo-GitHub%E6%90%AD%E5%BB%BA%E7%9A%84%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2-%E6%96%87%E4%BB%B6%E5%A4%87%E4%BB%BD%2F</url>
    <content type="text"><![CDATA[担心博客.md文件放在本地容易丢失，就新建了一个分支来备份。 第一次备份 在xxx.github.io文件夹, 右键选择Git Bash 进入命令行，进入项目所在目录，输入 touch .gitignore ，生成“.gitignore”文件。 在本地文件根目录中初始化 : git init 创建分支hexo ： git checkout -b hexo 提交到仓库，需要注意的事在提交之前要把themes目录下主题中的 .git 文件夹重命名或者删除，不然的话 git 会把主题当做子模块来处理。 12git add .git commit -m &apos;init&apos; 添加远程仓库 1git remote add origin git@github.com:MrWangwj/MrWangwj.github.io.git push 到远程分支 : git push origin hexo 日常的改动流程 依次执行git add .、git commit -m “…”、git push origin hexo指令将改动推送到GitHub（此时当前分支应为hexo） 然后才执行hexo g -d发布网站到master分支上。 虽然两个过程顺序调转一般不会有问题，不过逻辑上这样的顺序是绝对没问题的（例如突然死机要重装了，悲催….的情况，调转顺序就有问题了）。 在另一台电脑上使用 首先要克隆下这个项目 : git clone xxx 进入博客目录，切换到博客文件分支 ：git checkout -b hexo origin/hexo 安装hexo ： npm install hexo –save （不需要hexo init这条指令）]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阿里云服务器Ubuntu]]></title>
    <url>%2F2019%2F10%2F29%2F%E4%BD%BF%E7%94%A8WinSCP%E7%BB%99%E9%98%BF%E9%87%8C%E4%BA%91Ubuntu%E7%B3%BB%E7%BB%9F%E4%BC%A0%E8%BE%93%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[本地Windows环境上传文件到阿里云服务器（Ubuntu）WinSCP下载与安装下载链接：https://winscp.net/eng/download.php安装根据提示一步一步来即可 远程连接此处输入主机名和密码 用户名即可（均在阿里云服务器上的配置页面可见） 链接成功后如下图所示： 本地ubuntu系统上传文件到阿里云服务器（Ubuntu）把本地/home/xxx/xxx/xx.py文件传入到/home/download文件夹下，使用如下命令： 1scp /home/xxx/xxx/xx.py root@39.xxx.xx.xx:/home/download 然后输入服务器密码即可 阿里云ubantu下载anaconda3并配置jupyter notebook参考：https://www.jianshu.com/p/fff4a61dee7a 配置云服务器的安全组设置过程：云服务器管理控制台》云服务器ECS》网络和安全》安全组》配置规则》添加安全组规则授权对象这个我是填 0.0.0.0/0 。表示这个端口开放给所有ip 安装anaconda312345678910111213141516171819202122232425262728下载anaconda3清华镜像：sudo wget https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-5.0.1-Linux-x86_64.shsudo bash Anaconda3-5.0.1-Linux-x86_64.sh激活环境变量:source /etc/environment 添加Anaconda的TUNA镜像:conda config --add channels &apos;https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/&apos;设置搜索时显示通道地址：conda config --set show_channel_urls yes创建jupyter notebook运行环境，可以方便管理各类库conda create -n jupyter_notebook python=3激活环境source activate jupyter_notebook如果要退出环境的话，执行：source deactivate # 暂时不执行安装jupyter notebook (这个过程是接着激活环境后的)conda install jupyter notebook测试jupyter notebook --ip=127.0.0.1终端输出正常即可 配置jupyter notebook远程访问1234567891011121314151617181920212223242526272829303132jupyter notebook --generate-config生成文件后，文件在该目录下Writing default config to: /home/xx/.jupyter/jupyter_notebook_config.py打开jupyter_notebook_config.py文件（可以在winscp里面编辑这个文件）vim /home/xx/.jupyter/jupyter_notebook_config.py设置可以访问服务器的ip（在最后加上一行）c.NotebookApp.ip = &apos;*&apos;打开ipython：ipython调用passwd()函数生成密匙，把密匙复制下来，后面会有用：In [1]: from notebook.auth import passwdIn [2]: passwd()Enter password: Verify password: Out[2]: &apos;sha1:8361f5f08937:081cdf40730cb5548e2c213ddd36813a5313192f&apos;设置不在服务器端自动打开浏览器（继续在后面添加）：c.NotebookApp.password = &apos;sha1:8361f5f08937:081cdf40730cb5548e2c213ddd36813a5313192f&apos;c.NotebookApp.open_browser = False启动一下jupyter notebook是不是可以访问：jupyter notebook然后在自己电脑浏览器网址里输入：云服务器公网ip：8888大功告成！]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>阿里云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tensorflow & keras]]></title>
    <url>%2F2019%2F10%2F21%2Ftensorflow-2-0-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[友链： https://github.com/tensorflow/tensorflow https://github.com/tensorflow/tensorboard https://github.com/tensorflow/models https://tensorflow.google.cn/ tensorflow中文文档：https://tf.wiki/ https://github.com/dragen1860/TensorFlow-2.x-Tutorials keras中文文档：https://github.com/keras-team/keras-docs-zh https://github.com/keras-team/keras keras模型入门教程：40题刷爆Keras，人生苦短我选Keras 官方文档太辣鸡？TensorFlow 2.0开源工具书，30天「无痛」上手 想要系统学习tensorflow的小伙伴，推荐友链中的tensorflow中文文档和keras中文文档，这篇博客中的内容都摘自这俩中文文档，仅用来记录我自己的学习历程。 在 TensorFlow 2 中，即时执行模式将成为默认模式，无需额外调用 tf.enable_eager_execution() 函数（不过若要关闭即时执行模式，则需调用 tf.compat.v1.disable_eager_execution() 函数）。 TensorFlow 安装与环境配置conda虚拟环境： 12345conda create --name [env-name] # 建立名为[env-name]的Conda虚拟环境conda activate [env-name] # 进入名为[env-name]的Conda虚拟环境conda deactivate # 退出当前的Conda虚拟环境conda env remove --name [env-name] # 删除名为[env-name]的Conda虚拟环境conda env list # 列出所有Conda虚拟环境 TensorFlow基础python的with语句：with 语句适用于对资源进行访问的场合，确保不管使用过程中是否发生异常都会执行必要的“清理”操作，释放资源，比如文件使用后自动关闭、线程中锁的自动获取和释放等。 参考：https://www.ibm.com/developerworks/cn/opensource/os-cn-pythonwith/index.html 张量TensorFlow 使用 张量 （Tensor）作为数据的基本单位。TensorFlow 的张量在概念上等同于多维数组，我们可以使用它来描述数学中的标量（0 维数组）、向量（1 维数组）、矩阵（2 维数组）等各种量。 张量的重要属性是其形状、类型和值，可以通过张量的 shape 、 dtype 属性和 numpy() 方法获得。 TensorFlow 的大多数 API 函数会根据输入的值自动推断张量中元素的类型（一般默认为 tf.float32 ）。不过你也可以通过加入 dtype 参数来自行指定类型，例如 zero_vector = tf.zeros(shape=(2), dtype=tf.int32) 将使得张量中的元素类型均为整数。张量的 numpy() 方法是将张量的值转换为一个 NumPy 数组。 12345678910# 查看矩阵A的形状、类型和值print(A.shape) print(A.dtype) print(A.numpy()) tf.add(A, B) # 计算矩阵的和tf.matmul(A, B) # 计算矩阵A和B的乘积tf.square(x) # 对输入张量的每一个元素求平方，不改变张量形状tf.reduce_sum() # 对输入张量的所有元素求和，输出一个形状为空的纯量张量（可以通过 axis 参数来指定求和的维度，不指定则默认对所有元素求和） TensorFlow 的大多数 API 函数会根据输入的值自动推断张量中元素的类型（一般默认为 tf.float32 ）。不过你也可以通过加入 dtype 参数来自行指定类型，例如 zero_vector = tf.zeros(shape=(2), dtype=tf.int32) 将使得张量中的元素类型均为整数。张量的 numpy() 方法是将张量的值转换为一个 NumPy 数组。 自动求导机制TensorFlow 提供了强大的 自动求导机制 来计算导数。在即时执行模式下，TensorFlow 引入了 tf.GradientTape() 这个 “求导记录器” 来实现自动求导。 12345678910# 计算函数 y(x) = x^2 在 x = 3 时的导数：import tensorflow as tf# x 是一个初始化为3的 变量，使用 tf.Variable() 声明x = tf.Variable(initial_value=3.)# 在 tf.GradientTape() 的上下文内，所有计算步骤都会被记录以用于求导with tf.GradientTape() as tape: y = tf.square(x)y_grad = tape.gradient(y, x) # 计算y关于x的导数print([y, y_grad]) 与普通张量一样，变量同样具有形状、类型和值三种属性。使用变量需要有一个初始化过程，可以通过在 tf.Variable() 中指定 initial_value 参数来指定初始值。变量与普通张量的一个重要区别是其默认能够被 TensorFlow 的自动求导机制所求导，因此往往被用于定义机器学习模型的参数。 tf.GradientTape() 是一个自动求导的记录器。只要进入了 with tf.GradientTape() as tape 的上下文环境，则在该环境中计算步骤都会被自动记录。比如在上面的示例中，计算步骤 y = tf.square(x) 即被自动记录。离开上下文环境后，记录将停止，但记录器 tape 依然可用，因此可以通过 y_grad = tape.gradient(y, x) 求张量 y 对变量 x 的导数。 TensorFlow 下的线性回归 1234567891011121314151617181920212223242526272829303132333435import numpy as npX_raw = np.array([2013, 2014, 2015, 2016, 2017], dtype=np.float32)y_raw = np.array([12000, 14000, 15000, 16500, 17500], dtype=np.float32)# 归一化X = (X_raw - X_raw.min()) / (X_raw.max() - X_raw.min())print(&quot;X:&quot;,X)y = (y_raw - y_raw.min()) / (y_raw.max() - y_raw.min())print(&quot;y:&quot;,y)X = tf.constant(X)y = tf.constant(y)a = tf.Variable(initial_value=0.)b = tf.Variable(initial_value=0.)variables = [a, b]num_epoch = 10000# 声明一个梯度下降优化器（Optimizer），其学习率为 1e-3# 优化器可以帮助我们根据计算出的求导结果更新模型参数，从而最小化某个特定的损失函数，具体使用方式是调用其 apply_gradients() 方法optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)for e in range(num_epoch): # 使用tf.GradientTape()记录损失函数的梯度信息 with tf.GradientTape() as tape: y_pred = a * X + b loss = 0.5 * tf.reduce_sum(tf.square(y_pred - y)) # TensorFlow自动计算损失函数关于自变量（模型参数）的梯度 grads = tape.gradient(loss, variables) # TensorFlow自动根据梯度更新参数:需要提供参数 grads_and_vars，即待更新的变量variables及损失函数关于这些变量的偏导数grads optimizer.apply_gradients(grads_and_vars=zip(grads, variables))print(a, b) TensorFlow 模型建立与训练 模型的构建： tf.keras.Model 和 tf.keras.layers 模型的损失函数： tf.keras.losses 模型的优化器： tf.keras.optimizer 模型的评估： tf.keras.metrics Keras：model和layerKeras 是一个广为流行的高级神经网络 API，简单、快速而不失灵活性，现已得到 TensorFlow 的官方内置和全面支持。 Keras 有两个重要的概念： 模型（Model）和 层（Layer）。层将各种计算流程和变量进行了封装（例如基本的全连接层，CNN 的卷积层、池化层等），而模型则将各种层进行组织和连接，并封装成一个整体，描述了如何将输入数据通过各种层以及运算而得到输出。 Keras 模型类定义示意图 ： 通过模型类的方式编写线性模型： 1234567891011121314151617181920212223242526272829import tensorflow as tfX = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])y = tf.constant([[10.0], [20.0]])class Linear(tf.keras.Model): def __init__(self): super().__init__() self.dense = tf.keras.layers.Dense( units=1, activation=None, kernel_initializer=tf.zeros_initializer(), bias_initializer=tf.zeros_initializer() ) def call(self, input): output = self.dense(input) return output# 以下代码结构与前节类似model = Linear()optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)for i in range(100): with tf.GradientTape() as tape: y_pred = model(X) # 调用模型 y_pred = model(X) 而不是显式写出 y_pred = a * X + b loss = tf.reduce_mean(tf.square(y_pred - y)) grads = tape.gradient(loss, model.variables) # 使用 model.variables 这一属性直接获得模型中的所有变量 optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))print(model.variables) 多层感知机（MLP,多层全连接神经网络） 数据获取及预处理 MNISTLoader() 模型的构建 MLP(tf.keras.Model) 模型的训练：tf.keras.losses 和 tf.keras.optimizer 定义一些模型超参数 实例化模型和数据读取类，并实例化一个 tf.keras.optimizer 的优化器 从 DataLoader 中随机取一批训练数据； 将这批数据送入模型，计算出模型的预测值； 将模型预测值与真实值进行比较，计算损失函数（loss）。这里使用 tf.keras.losses 中的交叉熵函数作为损失函数； 计算损失函数关于模型变量的导数； 将求出的导数值传入优化器，使用优化器的 apply_gradients 方法更新模型参数以最小化损失函数 模型的评估： tf.keras.metrics 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556class MNISTLoader(): def __init__(self): mnist = tf.keras.datasets.mnist (self.train_data, self.train_label), (self.test_data, self.test_label) = mnist.load_data() # MNIST中的图像默认为uint8（0-255的数字）。以下代码将其归一化到0-1之间的浮点数，并在最后增加一维作为颜色通道 self.train_data = np.expand_dims(self.train_data.astype(np.float32) / 255.0, axis=-1) # [60000, 28, 28, 1] self.test_data = np.expand_dims(self.test_data.astype(np.float32) / 255.0, axis=-1) # [10000, 28, 28, 1] self.train_label = self.train_label.astype(np.int32) # [60000] self.test_label = self.test_label.astype(np.int32) # [10000] self.num_train_data, self.num_test_data = self.train_data.shape[0], self.test_data.shape[0] def get_batch(self, batch_size): # 从数据集中随机取出batch_size个元素并返回 index = np.random.randint(0, np.shape(self.train_data)[0], batch_size) return self.train_data[index, :], self.train_label[index]class MLP(tf.keras.Model): def __init__(self): super().__init__() self.flatten = tf.keras.layers.Flatten() # Flatten层将除第一维（batch_size）以外的维度展平 self.dense1 = tf.keras.layers.Dense(units=100, activation=tf.nn.relu) self.dense2 = tf.keras.layers.Dense(units=10) def call(self, inputs): # [batch_size, 28, 28, 1] x = self.flatten(inputs) # [batch_size, 784] x = self.dense1(x) # [batch_size, 100] x = self.dense2(x) # [batch_size, 10] output = tf.nn.softmax(x) return outputnum_epochs = 5batch_size = 50learning_rate = 0.001model = MLP()data_loader = MNISTLoader()optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)num_batches = int(data_loader.num_train_data // batch_size * num_epochs)for batch_index in range(num_batches): X, y = data_loader.get_batch(batch_size) with tf.GradientTape() as tape: y_pred = model(X) loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred) loss = tf.reduce_mean(loss) print(&quot;batch %d: loss %f&quot; % (batch_index, loss.numpy())) grads = tape.gradient(loss, model.variables) optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables)) sparse_categorical_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()num_batches = int(data_loader.num_test_data // batch_size)for batch_index in range(num_batches): start_index, end_index = batch_index * batch_size, (batch_index + 1) * batch_size y_pred = model.predict(data_loader.test_data[start_index: end_index]) sparse_categorical_accuracy.update_state(y_true=data_loader.test_label[start_index: end_index], y_pred=y_pred)print(&quot;test accuracy: %f&quot; % sparse_categorical_accuracy.result()) 交叉熵在 tf.keras 中，有两个交叉熵相关的损失函数 tf.keras.losses.categorical_crossentropy 和 tf.keras.losses.sparse_categorical_crossentropy 。其中 sparse 的含义是，真实的标签值 y_true 可以直接传入 int 类型的标签类别。具体而言： 1loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred) 与 1234loss = tf.keras.losses.categorical_crossentropy( y_true=tf.one_hot(y, depth=tf.shape(y_pred)[-1]), y_pred=y_pred) 的结果相同。 https://blog.csdn.net/tsyccnh/article/details/79163834 卷积神经网络（CNN）12345678910111213141516171819202122232425262728293031class CNN(tf.keras.Model): def __init__(self): super().__init__() self.conv1 = tf.keras.layers.Conv2D( filters=32, # 卷积层神经元（卷积核）数目 kernel_size=[5, 5], # 感受野大小 padding=&apos;same&apos;, # padding策略（vaild 或 same） activation=tf.nn.relu # 激活函数 ) self.pool1 = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2) self.conv2 = tf.keras.layers.Conv2D( filters=64, kernel_size=[5, 5], padding=&apos;same&apos;, activation=tf.nn.relu ) self.pool2 = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2) self.flatten = tf.keras.layers.Reshape(target_shape=(7 * 7 * 64,)) self.dense1 = tf.keras.layers.Dense(units=1024, activation=tf.nn.relu) self.dense2 = tf.keras.layers.Dense(units=10) def call(self, inputs): x = self.conv1(inputs) # [batch_size, 28, 28, 32] x = self.pool1(x) # [batch_size, 14, 14, 32] x = self.conv2(x) # [batch_size, 14, 14, 64] x = self.pool2(x) # [batch_size, 7, 7, 64] x = self.flatten(x) # [batch_size, 7 * 7 * 64] x = self.dense1(x) # [batch_size, 1024] x = self.dense2(x) # [batch_size, 10] output = tf.nn.softmax(x) return output 12345678910111213141516171819import tensorflow as tfimport tensorflow_datasets as tfdsnum_batches = 1000batch_size = 50learning_rate = 0.001dataset = tfds.load(&quot;tf_flowers&quot;, split=tfds.Split.TRAIN, as_supervised=True)dataset = dataset.map(lambda img, label: (tf.image.resize(img, [224, 224]) / 255.0, label)).shuffle(1024).batch(32)model = tf.keras.applications.MobileNetV2(weights=None, classes=5)optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)for images, labels in dataset: with tf.GradientTape() as tape: labels_pred = model(images) loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=labels, y_pred=labels_pred) loss = tf.reduce_mean(loss) print(&quot;loss %f&quot; % loss.numpy()) grads = tape.gradient(loss, model.trainable_variables) optimizer.apply_gradients(grads_and_vars=zip(grads, model.trainable_variables)) 循环神经网络（RNN）。。。 深度强化学习（DRL）。。。 TensorFlow 常用模块tf.train.Checkpoint 变量的保存与恢复 12345678910111213# train.py 模型训练阶段model = MyModel()# 实例化Checkpoint，指定保存对象为model（如果需要保存Optimizer的参数也可加入）checkpoint = tf.train.Checkpoint(myModel=model)# ...（模型训练代码）# 模型训练完毕后将参数保存到文件（也可以在模型训练过程中每隔一段时间就保存一次）checkpoint.save(&apos;./save/model.ckpt&apos;)# test.py 模型使用阶段model = MyModel()checkpoint = tf.train.Checkpoint(myModel=model) # 实例化Checkpoint，指定恢复对象为modelcheckpoint.restore(tf.train.latest_checkpoint(&apos;./save&apos;)) # 从文件恢复模型参数# 模型使用代码 tensorboard 训练过程可视化整体框架如下： 1234567summary_writer = tf.summary.create_file_writer(&apos;./tensorboard&apos;)# 开始模型训练for batch_index in range(num_batches): # ...（训练代码，当前batch的损失值放入变量loss中） with summary_writer.as_default(): # 希望使用的记录器 tf.summary.scalar(&quot;loss&quot;, loss, step=batch_index) tf.summary.scalar(&quot;MyScalar&quot;, my_scalar, step=batch_index) # 还可以添加其他自定义的变量 当我们要对训练过程可视化时，在代码目录打开终端（如需要的话进入 TensorFlow 的 conda 环境），运行: 1tensorboard --logdir=./tensorboard 然后使用浏览器访问命令行程序所输出的网址（一般是 http:// 计算机名称：6006），即可访问 TensorBoard 的可视界面。 tf.data 数据集的构建与预处理TFRecord TensorFlow 数据集存储格式tf.function 图执行模式TensorFlow 2 为我们提供了 tf.function 模块，结合 AutoGraph 机制，使得我们仅需加入一个简单的 @tf.function 修饰符，从而将模型转换为易于部署且高性能的 TensorFlow 图模型。 只需要将我们希望以图执行模式运行的代码封装在一个函数内，并在函数前加上 @tf.function 即可。 1234567891011121314151617181920212223242526272829import tensorflow as tfimport timefrom zh.model.mnist.cnn import CNNfrom zh.model.utils import MNISTLoadernum_batches = 1000batch_size = 50learning_rate = 0.001data_loader = MNISTLoader()model = CNN()optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)@tf.functiondef train_one_step(X, y): with tf.GradientTape() as tape: y_pred = model(X) loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred) loss = tf.reduce_mean(loss) # 注意这里使用了TensorFlow内置的tf.print()。@tf.function不支持Python内置的print方法 tf.print(&quot;loss&quot;, loss) grads = tape.gradient(loss, model.variables) optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))start_time = time.time()for batch_index in range(num_batches): X, y = data_loader.get_batch(batch_size) train_one_step(X, y)end_time = time.time()print(end_time - start_time) 一般而言，当模型由较多小的操作组成的时候， @tf.function 带来的提升效果较大。而当模型的操作数量较少，但单一操作均很耗时的时候，则 @tf.function 带来的性能提升不会太大。 附一个kaggle上mnist手写体识别的code： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475from __future__ import print_functionimport keras# from keras.datasets import mnistfrom keras.models import Sequentialfrom keras.layers import Dense, Dropoutfrom keras.optimizers import RMSpropimport numpy as npfrom keras.layers import Flattenfrom keras.layers import Conv2D, MaxPooling2Dfrom keras import backend as Kdef load_data(): path=&apos;/kaggle/input/mnist-data/mnist.npz&apos; f = np.load(path) x_train, y_train = f[&apos;x_train&apos;], f[&apos;y_train&apos;] x_test, y_test = f[&apos;x_test&apos;], f[&apos;y_test&apos;] f.close() return (x_train, y_train), (x_test, y_test)(x_train, y_train), (x_test, y_test) = load_data()batch_size = 128num_classes = 10epochs = 12# input image dimensionsimg_rows, img_cols = 28, 28if K.image_data_format() == &apos;channels_first&apos;: x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols) x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols) input_shape = (1, img_rows, img_cols)else: x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1) x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1) input_shape = (img_rows, img_cols, 1)x_train = x_train.astype(&apos;float32&apos;)x_test = x_test.astype(&apos;float32&apos;)x_train /= 255x_test /= 255print(&apos;x_train shape:&apos;, x_train.shape)print(x_train.shape[0], &apos;train samples&apos;)print(x_test.shape[0], &apos;test samples&apos;)# convert class vectors to binary class matricesy_train = keras.utils.to_categorical(y_train, num_classes)y_test = keras.utils.to_categorical(y_test, num_classes)model = Sequential()model.add(Conv2D(32, kernel_size=(3, 3), activation=&apos;relu&apos;, input_shape=input_shape))model.add(Conv2D(64, (3, 3), activation=&apos;relu&apos;))model.add(MaxPooling2D(pool_size=(2, 2)))model.add(Dropout(0.25))model.add(Flatten())model.add(Dense(128, activation=&apos;relu&apos;))model.add(Dropout(0.5))model.add(Dense(num_classes, activation=&apos;softmax&apos;))model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adadelta(), metrics=[&apos;accuracy&apos;])model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))score = model.evaluate(x_test, y_test, verbose=0)print(&apos;Test loss:&apos;, score[0])print(&apos;Test accuracy:&apos;, score[1]*100,&apos;%&apos;)]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kaggle]]></title>
    <url>%2F2019%2F10%2F19%2Fkaggle%2F</url>
    <content type="text"><![CDATA[友链： kaggle官网地址 从零开始，教初学者如何征战Kaggle竞赛-机器之心 Kaggle入门，看这一篇就够了 泰坦尼克号 Titanic: Machine Learning from DisasterKaggle链接：https://www.kaggle.com/c/titanic 逻辑回归应用之Kaggle泰坦尼克之灾:https://blog.csdn.net/han_xiaoyang/article/details/49797143 房价预测 House Prices: Advanced Regression Techniqueskaggle链接：https://www.kaggle.com/c/house-prices-advanced-regression-techniques 我的提交：https://www.kaggle.com/dccun1998/house-prices 手写数字识别 Digit Recognizerkaggle链接：https://www.kaggle.com/c/digit-recognizer 教程：https://www.kaggle.com/arthurtok/interactive-intro-to-dimensionality-reduction 手写数字识别 Kannada MNISTkaggle链接：https://www.kaggle.com/c/Kannada-MNIST 我的提交（keras）：https://www.kaggle.com/dccun1998/kannada-mnist-competition]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>kaggle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apktool等工具的使用]]></title>
    <url>%2F2019%2F10%2F14%2FAndrowarn%2F</url>
    <content type="text"><![CDATA[Android tools github地址：https://github.com/maoqyhz/DroidCC/blob/master/tools.md下面是几个Android恶意代码检测用到的工具，总结一下下载方法和使用方法，安装环境为ubantu18.04。 apktool：是apk格式文件与smali文件的转换dex2jar：是dex格式文件与jar文件的转换smali/baksmali：是dex格式文件与smali文件的转换 Androguardandroguard主要用来进行静态分析，其默认采用ded作为反编译的软件，同时提供了很多模块供分析人员使用。 学习链接： github地址: https://github.com/androguard/androguard 项目主页: https://code.google.com/archive/p/androguard/ 使用文档: https://androguard.readthedocs.io/en/latest/index.html 附一个中文版Androguard使用说明： https://www.jianshu.com/p/079e40800ef4 安装: 1pip install -U androguard 使用： 命令行输入:androguard analyze 或者用jupyter notebook，当然其他IDE也行，下载好androguard后他就相当于python的一个包，可以直接导入：import androguard 123from androguard.misc import AnalyzeAPKa, d, dx = AnalyzeAPK(&quot;sample.apk&quot;)print(a,d,dx) 关于Androguard的项目： AndroPyTool AndrowarnAndrowarn是一款专为Android端应用程序设计的安全分析工具，主要功能是检测并提醒用户Android应用程序中潜在的恶意行为。在androguard库的帮助下，Androwarn可以通过对目标应用程序的Dalvik字节码和Smali代码进行静态分析，来判断目标应用程序中潜在的恶意行为。分析完成之后，工具会自动生成分析报告，报告中的技术细节划分，取决于用户的设置参数。 学习链接： github链接: https://github.com/maaaaz/androwarn/ 附一个中文版教程链接: https://www.freebuf.com/sectool/199407.html 安装： 12pip install androwarn# Or git clone that repository and ：pip install -r requirements.txt 使用： 1python androwarn.py -i my_application_to_be_analyzed.apk -r html -v 3 把github仓库clone到本地，cd 该文件夹，打开终端，把上面命令行的my_application_to_be_analyzed.apk换成你自己要分析的apk文件的名字，就会在该文件夹生成一个html分析文件。如下图所示： ApktoolAPKTool是GOOGLE提供的APK编译工具，需要JAVA运行环境，是逆向分析工具. 学习链接： 博客教程: https://ibotpeaches.github.io/Apktool/ github地址：https://github.com/iBotPeaches/Apktool github地址2：https://github.com/iBotPeaches/Apktool/tree/gh-pages 安装： 下载 Linux wrapper script 链接 (右击，保存为 apktool) 下载 apktool-2 链接 (保存为apktool.jar) 移动文件 (apktool.jar &amp; apktool) 到 /usr/local/bin（sudo cp -i aapt apktool apktool.jar /usr/local/bin/ 基本上到这一步就结束了） 保证文件可执行 (chmod +x) 在命令行运行apktool 使用： decode：该命令用于进行反编译apk文件apktool d file.apk dirfile.apk：代表了要反编译的apk文件的路径，最好写绝对路径dir：代表了反编译后的文件的存储位置 build：该命令用于编译修改好的文件apktool b dir 这里的dir就是刚才你反编译时输入的dir DroidboxDroidBox是用来动态分析Android应用行为的工具。 学习链接： 官网链接：https://code.google.com/archive/p/droidbox/ github地址：https://github.com/pjlantz/droidbox 安装：（参考：https://blog.csdn.net/u012195899/article/details/52814013） Oracle JDK（在本博客《ubantu配置》一文中写了jdk安装教程） 安装sdk：linux 服务器下载：wget http://dl.google.com/android/android-sdk_r24.4.1-linux.tgz将压缩包解压到/home/android/sdk 目录下修改环境变量：export PATH=/home/name/Android/Sdk/android-sdk-linux/tools:$PATHexport PATH=/home/name/Android/Sdk/android-sdk-linux/platform-tools:$PATHsource ~/.bashrc sdk、avd管理/home/dccun/Android/Sdk/android-sdk-linux/tools目录下：终端输入android即可进入sdk管理，可下载avd安装完成之后，大家可以在命令行输入一下命令来查看当前的安装版本：android list targets下载完之后create虚拟机点击OK创建完毕。这个时候大家可以先运行一下avd是否能够正常开机。我的虚拟机名称是droid，因此命令如下：emulator -avd droid如果成功运行，那么恭喜，你的sdk部分已经配置完成！如果想要删除某个虚拟机，使用的命令为:android delete avd -n avd_name。android list target #查看可获取的安卓虚拟机android list avd #查看已创建的安卓虚拟机 下载最新版的DroidBoxhttps://github.com/pjlantz/droidbox/releases ，这里我下载的时候最新版为：DroidBox411RC.tar.gz进入droidbox目录下：启动虚拟机：./startemu.sh droid开始分析：./droidbox.sh 1.apk（apk使用绝对路径）可以设置分析时间：./droidbox.sh 1.apk 10 (10表示10s) 在虚拟机下载apk把apk放到/Android/Sdk/android-sdk-linux/platform-tools目录下，在这个目录下进入终端，输入adb install sample.apk即可，实践效果如下图所示： 出现的问题及解决方法： Linux系统(Ubuntu)下AndroidStudio创建AVD虚拟器出现“/dev/kvm is missing”参考：https://blog.csdn.net/lpcrazyboy/article/details/80270816(1)进入BIOS里，把Virtualization Technology(VT)的状态由Disable改为Enable。（这个在cpu模块里找）(2)打开终端，输入：sudo apt-get install qemu-kvm(3)安装完成后，输入：sudo kvm-ok ubuntu18.04系统下androidstudio启动模拟器发生错误: /dev/kvm device:permission denied参考：https://blog.csdn.net/blackei/article/details/84559180在终端输入：sudo chown username -R /dev/kvm将username替换成自己电脑当前登录的用户名称即可。如当前电脑登录的是android，那么在终端输入的则为：sudo chown android -R /dev/kvm JD-GUI学习链接 https://github.com/java-decompiler/jd-gui http://java-decompiler.github.io/ dex2jardex2jar是将apk中的java源码编译生成的java字节码文件反编译成java源码。 学习链接 https://github.com/pxb1988/dex2jar 下载：下载链接 使用： 1sh d2j-dex2jar.sh -f ~/path/to/apk_to_decompile.apk 运行结束后会在d2j-dex2jar.sh所在文件夹下生成apk_to_decompile-dex2jar.jar文件 smali/baksmalismali是将Android字节码用可阅读的字符串形式表现出来的一种语言,可以称之为Android字节码的反汇编语言。 学习链接 https://github.com/JesusFreke/smali 逆向之Smali入门学习 下载：下载链接]]></content>
      <categories>
        <category>毕设</category>
      </categories>
      <tags>
        <tag>android-tool</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[「面试」商汤Java开发实习面试]]></title>
    <url>%2F2019%2F10%2F03%2F%E5%95%86%E6%B1%A4Java%E5%BC%80%E5%8F%91%E5%AE%9E%E4%B9%A0%E9%9D%A2%E8%AF%95%2F</url>
    <content type="text"><![CDATA[商汤Java实习面试 19.9.25== 与 equals== : 它的作用是判断两个对象的地址是不是相等。即，判断两个对象是不是同一个对象。(基本数据类型==比较的是值，引用数据类型==比较的是内存地址)equals() : 它的作用也是判断两个对象是否相等。但它一般有两种使用情况： 情况1：类没有覆盖 equals() 方法。则通过 equals() 比较该类的两个对象时，等价于通过“==”比较这两个对象。 情况2：类覆盖了 equals() 方法。一般，我们都覆盖 equals() 方法来两个对象的内容相等；若它们的内容相等，则返回 true (即，认为这两个对象相等)。另外，当创建 String 类型的对象时，虚拟机会在常量池中查找有没有已经存在的值和要创建的值相同的对象，如果有就把它赋给当前引用。如果没有就在常量池中重新创建一个 String 对象。 好像还问了一个类怎么重写equals()方法。 12345678910111213public boolean equals(Object otherObject)&#123; //测试两个对象是否是同一个对象，是的话返回true if(this == otherObject) &#123; //测试检测的对象是否为空，是就返回false return true; &#125; if(otherObject == null) &#123; //测试两个对象所属的类是否相同，否则返回false return false; &#125; if(getClass() != otherObject.getClass()) &#123; //对otherObject进行类型转换以便和类A的对象进行比较 return false; &#125; A other=(A)otherObject; return Object.equals(类A对象的属性A，other的属性A）&amp;&amp;类A对象的属性B==other的属性B……; &#125; final 关键字的用法final关键字主要用在三个地方：变量、方法、类。 对于一个final变量，如果是基本数据类型的变量，则其数值一旦在初始化之后便不能更改；如果是引用类型的变量，则在对其初始化之后便不能再让其指向另一个对象。 当用final修饰一个类时，表明这个类不能被继承。final类中的所有成员方法都会被隐式地指定为final方法。 使用final方法的原因有两个。第一个原因是把方法锁定，以防任何继承类修改它的含义；第二个原因是效率。在早期的Java实现版本中，会将final方法转为内嵌调用。但是如果方法过于庞大，可能看不到内嵌调用带来的任何性能提升（现在的Java版本已经不需要使用final方法进行这些优化了）。类中所有的private方法都隐式地指定为fianl。 自动装箱与拆箱 装箱：将基本类型用它们对应的引用类型包装起来； 拆箱：将包装类型转换为基本数据类型。 具体的问了一下int和Integer,像下面这样，面试官说代码,让我直接说结果哪个跟哪个用==还是equals相等还是不等。 Integer是int的包装类，int则是java的一种基本数据类型 Integer变量必须实例化后才能使用，而int变量不需要 Integer实际是对象的引用，当new一个Integer时，实际上是生成一个指针指向此对象；而int则是直接存储数据值 Integer变量和int变量比较时，只要两个变量的值是向等的，则结果为true（因为包装类Integer和基本数据类型int比较时，java会自动拆包装为int，然后进行比较，实际上就变为两个int变量的比较） 由于Integer变量实际上是对一个Integer对象的引用，所以两个通过new生成的Integer变量永远是不相等的（因为new生成的是两个对象，其内存地址不同）。 非new生成的Integer变量和new Integer()生成的变量比较时，结果为false。（因为非new生成的Integer变量指向的是java常量池中的对象，而new Integer()生成的变量指向堆中新建的对象，两者在内存中的地址不同） 对于两个非new生成的Integer对象，进行比较时，如果两个变量的值在区间-128到127之间，则比较结果为true，如果两个变量的值不在此区间，则比较结果为false Integer的默认值是null，int的默认值是0 123456789101112131415161718public static void main(String[] args) &#123; Integer i = 10; Integer j = 10; System.out.println(i == j); Integer a = 128; Integer b = 128; System.out.println(a == b); int k = 10; System.out.println(k == i); int kk = 128; System.out.println(kk == a); Integer m = new Integer(10); Integer n = new Integer(10); System.out.println(m == n);&#125; 正确答案：Integer a = 128; 通过反编译工具生成的class文件是：Integer a = Integer.valueOf(128);这就是基本数据类型的自动装箱，128是基本数据类型，然后被解析成Integer类。 Integer a = new Integer(128);int m = a;反编译生成的class文件：Integer a = new Integer(128);int m = a.intValue(); 简单来讲：自动装箱就是Integer.valueOf(int i);自动拆箱是i.intValue(); 123456789101112131415Integer a = 1;Integer b = 2;Integer c = 3;Integer d = 3;Integer e = 321;Integer f = 321;Long g = 3L;Long h = 2L;System.out.println(c == d); //tSystem.out.println(e == f); //fSystem.out.println(c == (a + b)); //tSystem.out.println(c.equals((a+b)));//tSystem.out.println(g == (a+b));//tSystem.out.println(g.equals(a+b));//fSystem.out.println(g.equals(a+h));//t 线程的五大状态及其常用方法（1）新建状态：即单纯地创建一个线程，创建线程有三种方式（2）就绪状态：在创建了线程之后，调用Thread类的start()方法来启动一个线程，即表示线程进入就绪状态（3）运行状态：当线程获得CPU时间，线程才从就绪状态进入到运行状态（4）阻塞状态：线程进入运行状态后，可能由于多种原因让线程进入阻塞状态，如：调用sleep()方法让线程睡眠，调用wait()方法让线程等待，调用join()方法、suspend()方法（它现已被弃用！）以及阻塞式IO方法（5）死亡状态：run()方法的正常退出就让线程进入到死亡状态，还有当一个异常未被捕获而终止了run()方法的执行也将进入到死亡状态 线程等待——wait()方法 线程唤醒——-notify()方法 sleep()使线程休眠一段时间，将处于阻塞状态;如果调用了sleep方法之后，没有其他等待执行的线程，这个时候线程不会恢复执行，还是在休眠。 join()阻塞指定线程等到另外一个线程完成后继续执行。 yield()让当前正在执行的线程暂停，不是阻塞线程，而是将线程的运行态转入就绪态；调用了该方法后，如果没有其他线程等待执行，此时当前线程就会马上恢复执行。 setDaemon()可以将指定的线程设置成后台线程，守护线程；创建用户线程的线程结束时，后台线程也随之消亡；只能在线程启动之前把它设为后台线程。 setPriority(int newPriority) getPriority()线程的优先级代表的是概率；范围从1到10，默认为5。 stop()终止线程运行，变为死亡态； Java垃圾回收机制（GC） 垃圾回收(Garbage Collection)是Java虚拟机(JVM)垃圾回收器提供的一种用于在空闲时间不定时回收无任何对象引用的对象占据的内存空间的一种机制。 垃圾：无任何对象引用的对象。 回收：清理“垃圾”占用的内存空间而非对象本身。 发生地点：一般发生在堆内存中，因为大部分的对象都储存在堆内存中。 发生时间：程序空闲时间不定时回收。 哪些内存需要回收？(对象是否可以被回收的两种经典算法: 引用计数法 和 可达性分析算法) 什么时候回收？ （堆的新生代、老年代、永久代的垃圾回收时机，MinorGC 和 FullGC） 如何回收？(三种经典垃圾回收算法(标记清除算法、复制算法、标记整理算法)及分代收集算法 和 七种垃圾收集器) 最后：你有什么问题问我吗？]]></content>
      <categories>
        <category>面试</category>
      </categories>
      <tags>
        <tag>大四实习面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu配置]]></title>
    <url>%2F2019%2F09%2F30%2Fubantu%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[今天装了ubuntu18.04,记录一下安装完ubuntu之后做的事情，和使用ubuntu的过程中遇到的问题、解决方法、小技巧。 ubuntu是基于debian开发的，centos是基于redhat开发的，所以ubuntu可以使用deb结尾的包，而centos使用rpm结尾的包。 显示文件夹中隐藏的文件：ctrl+H。 查看Ubuntu显卡的型号： 1lspci|grep VGA 查看CPU 1lscpu usr内文件夹的删除复制移动 123456一：复制移动sudo cp 要复制的文件 /usr/local/bin（目的文件夹）sudo mv 要移动的文件 /usr/local/bin（目的文件夹）二：删除sudo su #进入root下rm -rf 要删除的文件 linux对文件赋权限的命令chmod参考：https://www.cnblogs.com/insane-Mr-Li/p/10716293.html使用方式 : chmod [-cfvR] [–help] [–version] mode file… 卸载软件参考：https://blog.csdn.net/luckydog612/article/details/80877179输入dpkg –list终端输出电脑上安装的所有软件在终端上输入命令sudo apt-get –purge remove 包名（–purge是可选项，写上这个属性是将软件及其配置文件一并删除，如不需要删除配置文件，可执行sudo apt-get remove 包名） Ubuntu配置环境变量的两种常用方法（.bashrc和/etc/profile）参考：https://blog.csdn.net/yiminghd2861/article/details/98854882 双系统安装过程记录参考: https://blog.csdn.net/weixin_43538911/article/details/99647086 卸载ubuntu参考：https://blog.csdn.net/lele_god/article/details/109046860 ubuntu下可删除哪些文件来释放系统空间？参考：https://zhidao.baidu.com/question/166531881.html 安装好Ubuntu18.04之后要做的事大全参考: https://blog.csdn.net/haeasringnar/article/details/81809040要安装搜狗拼音、git可以按照这个里面的教程，下面补充几个更好用的按照教程 安装搜狗拼音如果按照上面一条安装后，因为个人失误导致搜狗拼音不能用，可以参考下面教程：https://blog.csdn.net/qq_33159059/article/details/85019467 12345678910111213# 先卸载掉fcitx，及其所有相关的软件sudo apt -y remove *fcitx*# 然后来个彻底清除sudo apt autoremove# 安装了下面的这部分，搜狗输入法就可以使用了，不过候选区没有背景，是透明的sudo apt -y install fcitx fcitx-bin fcitx-table fcitx-table-all# 安装fcitx可视化的配置界面sudo apt -y install fcitx-config-gtk# 就安装gtk，不要安装gtk2。因为gtk2的配置界面没有gtk的强大，而且同时安装，也只有gtk的生效sudo dpkg -i sogoupinyin_2.2.0.0108_amd64.deb进入“语言支持”界面，进行输入法框架的配置：![upload successful](/images/pasted-10.png)最后重启，确保设置都已生效 安装 Anaconda3参考: https://blog.csdn.net/qq_15192373/article/details/81091098 1234567在清华大学开源软件镜像站下载：https://mirrors.tuna.tsinghua.edu.cn/help/anaconda/bash Anaconda3-5.2.0-Linux-x86_64.sh若在终端输入 python，仍然会显示Ubuntu自带的python版本，我们执行：sudo gedit ~/.bashrc在最后一行添加：export PATH=&quot;/home/xxx/anaconda3/bin:$PATH&quot; 问题：pip命令报错Traceback (most recent call last): File “/usr/bin/pip”解决：https://blog.csdn.net/lyll616/article/details/85090132 12345打开终端，在终端中输入 ： sudo gedit /usr/bin/pip插入或者修改为：from pip import __main__ //修改if __name__ == &apos;__main__&apos;: sys.exit(__main__._main())//修改 安装Mac主题https://zhuanlan.zhihu.com/p/71588449尝试了一下效果不错 安装MySQL8.0参考:https://www.cnblogs.com/luoli-/p/9249769.htmlubuntu18.04首次登录mysql未设置密码或忘记密码解决方法:https://blog.csdn.net/qq_38737992/article/details/81090373 1234567下载deb包：https://dev.mysql.com/downloads/repo/apt/sudo dpkg -i mysql-apt-config_0.8.10-1_all.debsudo apt updatesudo apt install mysql-server密码加密方式选择5.x查看mysql是否安装成功：mysql -u root -p查看mysql字符集，mysql8字符集默认为utf-8：show variables like &apos;%char%&apos;; 安装mendeley参考:https://www.mendeley.com/guides/download-mendeley-desktop/ubuntu/instructions 12在上面链接中下载deb包sudo dpkg -i &lt;path-to-downloaded-package&gt; 在使用过程中出现不能输入中文的现象，解决方法：https://blog.csdn.net/weixin_40100431/article/details/82633423 12345首先在终端中定位一个文件的位置locate libfcitxplatforminputcontextplugin.so然后将上述文献拷贝到mendeley的安装路径当中sudo cp /usr/lib/x86_64-linux-gnu/qt5/plugins/platforminputcontexts/libfcitxplatforminputcontextplugin.so /opt/mendeleydesktop/plugins/qt/plugins/platforminputcontexts/关闭mendeley重新启动即可。 安装配置JAVA环境参考:https://blog.csdn.net/weixx3/article/details/80296779 1234567891011121314去oracle官网下载：https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html解压后，将文件从下载目录挪到/usr/local下sudo mv jdk1.8.0_171 /usr/local/jdk1.8修改全局配置文件，作用于所有用户：vim /etc/profileexport JAVA_HOME=/usr/local/jdk1.8export JRE_HOME=$&#123;JAVA_HOME&#125;/jreexport CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/libexport PATH=.:$&#123;JAVA_HOME&#125;/bin:$PATH修改完后保存并退出，按下Esc后输入:q!即可使修改的配置立刻生效 ： source /etc/profile 检查是否安装成功：java -version 安装wps安装按照上面《安装好Ubuntu18.04之后要做的事大全》一文，字体缺失按照下面教程：WPS for Linux（ubuntu）字体配置(字体缺失解决办法)：https://www.cnblogs.com/liangml/p/5969404.html 1234567891011在官网下载wps：https://www.wps.cn/product/wpslinux/安装wps：sudo dpkg -i wps*.deb 安装wps下载缺失字体：国内下载地址：https://pan.baidu.com/s/1eS6xIzo下载完成后，解压并进入目录中，继续执行：sudo cp * /usr/share/fontssudo mkfontscalesudo mkfontdir运行fc-cache命令更新字体缓存：sudo fc-cache重启wps即可，字体缺失的提示不再出现。 安装node、npm、hexo参考：https://www.cnblogs.com/guanine/p/9392411.htmlhttps://blog.csdn.net/Iversonx/article/details/82807598 12345678键入以下内容刷新本地包索引：sudo apt update从存储库安装Node.js：sudo apt install nodejs需要额外安装npm，你可以通过输入以下命令来完成sudo apt install npm 安装hexosudo npm install --unsafe-perm --verbose -g hexo 安装网易云音乐参考： https://blog.csdn.net/weixin_43693233/article/details/90685359第一次下载网易云音乐，点击图标打不开，只能用sudo进入，很麻烦，百度也看到好多人都是这种问题，上面这个教程下载的网易云是点击图标可以直接进入的，推荐！ 安装Android studio参考：https://blog.csdn.net/Sacredness/article/details/86514460我并没有安装SDK，百度了一下有的博客说android studio 可以默认帮你安装android-sdk。 下载地址：https://developer.android.google.cn/studio/#downloads将压缩包解压到要安装的位置在软件的bin目录下打开命令行，执行命令bash studio.sh.（如果你需要root权限，执行sudo bash studio.sh）创建桌面图标：至于怎么新建一个项目，参考：https://blog.csdn.net/Sacredness/article/details/82929768 安装pycharm同上 安装idea参考：https://blog.csdn.net/lishundi/article/details/82762532 12345下载地址: https://www.jetbrains.com/idea/download/#section=linux 解压到/opt下：sudo tar -zxvf ideaIC-2018.2.3-no-jdk.tar.gz -C /opt进入到opt位置：cd /opt/进入到IDEA文件夹下的bin目录：cd /opt/idea-IC-182.4323.46/bin启动 IDEA：./idea.sh 截图工具flameshot参考：https://www.jianshu.com/p/e1678c1d175d 1sudo apt-get install flameshot 设置&gt;设备&gt;键盘，设置一个自定义快捷键（拉到最下面）命令填写：flameshot gui 截完图后保存Ctrl+S，复制到剪贴板 Ctrl+C]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[加密文章]机器学习深度学习笔记]]></title>
    <url>%2F2019%2F09%2F22%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[友链: 吴恩达老师的机器学习课程个人笔记： https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes 吴恩达老师的深度学习课程笔记及资源： https://github.com/fengdu78/deeplearning_ai_books github上一个100天学习机器学习的项目： https://github.com/MLEveryday/100-Days-Of-ML-Code https://github.com/Avik-Jain/100-Days-Of-ML-Code 深度学习框架 全世界最为流行的深度学习框架有PaddlePaddle、Tensorflow、Caffe、Theano、MXNet、Torch和PyTorch。conda是一个流行的Python包管理软件。 特征缩放 特征缩放(feature scaling)梯度下降算法中，在有多个特征的情况下，如果能确保这些不同的特征都处在一个相近的范围，这样梯度下降法就能更快地收敛。 MNIST Dataset MNIST Dataset Introductionmnist数据集里的每张图片大小为28 * 28像素，可以用28*28的大小的数组来表示一张图片。标签用大小为10的数组来表示，这种编码我们称之为One hot（独热编码）。 One-hot编码 One-hot编码（独热编码）独热编码使用N位代表N种状态，任意时候只有其中一位有效。采用独热编码的例子性别:[0, 1]代表女，[1, 0]代表男数字0-9:[0,0,0,0,0,0,0,0,0,1]代表9，[0,1,0,0,0,0,0,0,0,0]代表1 损失函数（loss function） 常见的损失函数定义: 差的平方和 sum((y - label)^2) 交叉熵 -sum(label * log(y))(交叉熵只关注独热编码中有效位的损失) 梯度下降learning_rate:学习速率 softmax激活函数 作用:一是放大效果，二是梯度下降时需要一个可导的函数。 1234def softmax(x): import numpy as np return np.exp(x) / np.sum(np.exp(x), axis=0)softmax([4, 5, 10]) tensorflow playground附上一张我试着训练出来的loss最小的螺旋神经网络 混淆矩阵混淆矩阵也称误差矩阵，是表示精度评价的一种标准格式，用n行n列的矩阵形式来表示。在人工智能中，混淆矩阵（confusion matrix）是可视化工具，特别用于监督学习，在无监督学习一般叫做匹配矩阵。 KL散度、JS散度、Wasserstein距离 梯度下降法（BGD &amp; SGD &amp; Mini-batch SGD）梯度下降法（Gradient Descent） 优化思想：用当前位置的负梯度方向作为搜索方向，亦即为当前位置下降最快的方向。越接近目标值时，步长越小，下降越慢，梯度下降不一定能找到全局最优解，可能寻找到的是局部最优解。（当损失函数是凸函数时，梯度下降得到的解一定是全局最优解，因为凸函数的极小值即为最小值） 批量梯度下降法（Batch Gradient Descent，BGD）：每次是对整个训练集进行梯度下降。 随机梯度下降法（Stochastic Gradient Descent，SGD）：每次只对一个样本进行梯度下降。 小批量梯度下降法（Mini-batch Gradient Desent，也称Mini-batch SGD）：每次处理样本的个数在上面二者之间，把整个大的训练集划分为若干个小的训练集，在处理完整个训练集之前，先让梯度下降法处理一部分数据，那么算法就会相对快一些。当数据集很大时，使用Mini-batch SGD更新参数更快，有利于更鲁棒地收敛，避免局部最优。 选择Mini-batch SGD的参数 batch size 不难看出Mini-batch SGD的 batch 大小，也是一个影响着算法效率的参数。 如果训练集较小，一般小于2000的，就直接使用BGD。 一般Mini-batch SGD的大小在 64 到 512 之间，选择 2 的 n 次幂会运行得相对快一些。注意这个值设为 2 的 n 次幂，是为了符合cpu gpu的内存要求，如果不符合的话，不管用什么算法表现都会很糟糕。 深度学习中的batch Batch_Size：批尺寸 Normalization模型 在机器学习领域有个很重要的假设：IID独立同分布假设，也就是假设训练数据和测试数据是满足相同分布的，这是通过训练数据获得的模型能够在测试集上获得好的效果的一个基本保障。在深度学习网络中，后一层的输入是受前一层的影响的，而为了方便训练网络，我们一般都是采用Mini-Batch SGD来训练网络的（Mini-Batch SGD的两个优点是：梯度更新方向更准确和并行计算速度快）。 我们知道在神经网络训练开始前，都要对输入数据做一个归一化处理，那么具体为什么需要归一化呢？归一化后有什么好处呢？原因在于神经网络学习过程本质就是为了学习数据分布，一旦训练数据与测试数据的分布不同，那么网络的泛化能力也大大降低；另外一方面，一旦每批训练数据的分布各不相同(batch 梯度下降)，那么网络就要在每次迭代都去学习适应不同的分布，这样将会大大降低网络的训练速度，这也正是为什么我们需要对数据都要做一个归一化预处理的原因。 对于深度网络的训练是一个复杂的过程，只要网络的前面几层发生微小的改变，那么后面几层就会被累积放大下去。一旦网络某一层的输入数据的分布发生改变，那么这一层网络就需要去适应学习这个新的数据分布，所以如果训练过程中，训练数据的分布一直在发生变化，那么将会影响网络的训练速度。 除了输入层的数据外(因为输入层数据，我们已经人为的为每个样本归一化)，后面网络每一层的输入数据分布是一直在发生变化的，因为在训练的时候，前面层训练参数的更新将导致后面层输入数据分布的变化。以网络第二层为例：网络的第二层输入，是由第一层的参数和input计算得到的，而第一层的参数在整个训练过程中一直在变化，因此必然会引起后面每一层输入数据分布的改变。 我们把网络中间层在训练过程中，数据分布的改变称之为：“Internal Covariate Shift”。Internal指的是深层网络的隐层，是发生在网络内部的事情，而不是covariate shift问题只发生在输入层。Batch Normalization就是来解决该问题的。Batch Normalization的基本思想就是能不能让每个隐层节点的激活输入分布固定下来，从而避免Internal Covariate Shift的问题。 BN(Batch Normalization)属于网络的一层。BN的基本思想其实相当直观：因为深层神经网络在做非线性变换前的激活输入值（就是那个x=WU+B，U是输入）随着网络深度加深或者在训练过程中，其分布逐渐发生偏移或者变动，之所以训练收敛慢，一般是整体分布逐渐往非线性函数的取值区间的上下限两端靠近（对于Sigmoid函数来说，意味着激活输入值WU+B是大的负值或正值），所以这导致后向传播时低层神经网络的梯度消失，这是训练深层神经网络收敛越来越慢的本质原因，而BN就是通过一定的规范化手段，把每层神经网络任意神经元输入值的分布强行拉回到均值为0方差为1的标准正态分布，使得激活输入值落在非线性函数对输入比较敏感的区域，这样输入的小变化就会导致损失函数较大的变化，意思是这样让梯度变大，避免梯度消失问题产生，而且梯度变大意味着学习收敛速度快，能大大加快训练速度。 理解dropout参考：http://blog.csdn.net/stdcoutzyx/article/details/49022443 dropout是指在深度学习网络的训练过程中，对于神经网络单元，按照一定的概率将其暂时从网络中丢弃。注意是暂时，对于随机梯度下降来说，由于是随机丢弃，故而每一个mini-batch都在训练不同的网络。 dropout率的选择： 经过交叉验证，隐含节点dropout率等于0.5的时候效果最好，原因是0.5的时候dropout随机生成的网络结构最多。 dropout也可以被用作一种添加噪声的方法，直接对input进行操作。输入层设为更接近1的数。使得输入变化不会太大（0.8）]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>DL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019电子竞赛准备和比赛过程记录]]></title>
    <url>%2F2019%2F08%2F07%2F2019%E7%94%B5%E5%AD%90%E7%AB%9E%E8%B5%9B%E5%9B%BD%E8%B5%9B%2F</url>
    <content type="text"><![CDATA[比赛时间：2019.8.7-2019.8.10 7月31日（没有时间仔细写笔记，只做简单记录，仅供自己比赛使用） 复习PID算法五月份的时候练习2017年国赛滚球系统已经了解过PID算法，但是并不熟练，比赛前还是有必要再进一步熟悉PID算法的。滚球经验总结 摄像头openmv17年国赛控制题滚球系统用到了摄像头的图像处理，老师预测19年还会用到图像处理，因此我们选择了openmv。跟ov系列摄像头比起来，openmv实在是太好用了，不用再拘泥于C，可以使用python。上面滚球经验总结里面也有写openmv跟stm32f407战舰版通信方法。openmv主要就是在官网学习的。 接线方式：openmv——407核心板P4————RX（PA10_USART1）P5————TX（PA9_USART1）GND———GND MPU6050六轴传感器MPU6050 采用 IIC 与 STM32F4 通信。姿态数据，也就是欧拉角——航向角（yaw）、横滚角（roll）、俯仰角（pitch）。MPU6050 自带了数字运动处理器，即 DMP。检测轴及方向： MPU6050 与探索者 STM32F4 开发板的连接： 8月1日今天台风登陆小岛~一直到上午十点多才来电，中午出了器材清单，开始猜题。 8月2日今天一天就是看网上各种猜题，淘宝找做控制类的器材，报给学校一份，自己买了一份备用（主要因为学校买的各种不靠谱）。 8月3日SD卡的使用openmv——SD卡先插SD卡再上电SD卡的文件系统会自动取代内置的Flash文件系统openmv使用SD卡的情况： 代码行数太多 存储图片 录制视频 stm32——SD卡只运行了stm32自带的例程，并不懂SD卡怎么用。。。 通信通信很重要，当初做滚球，openmv和stm32通信传输小球坐标时费了很大劲，通信还是要好好看看。 蓝牙模块HC-05 参考博客主要想实现的功能是手机蓝牙控制STM32——手机通过蓝牙传输到HC-05上，再通过串口通信和STM32通信。我们组在网上买了一个组装好的小车，手机APP通过蓝牙HC06控制，蓝牙名称BT04，密码1234。测试APP：蓝牙模块通过stlink连接电脑，用串口调试助手观察按下APP的按键时接收的信息。蓝牙串口APP 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859#include &quot;stm32f10x.h&quot; void My_USART1_Init(void) &#123; GPIO_InitTypeDef GPIO_InitStrue; USART_InitTypeDef USART_InitStrue; NVIC_InitTypeDef NVIC_InitStrue; RCC_APB2PeriphClockCmd(RCC_APB2Periph_GPIOA,ENABLE);//GPIO端口使能 RCC_APB2PeriphClockCmd(RCC_APB2Periph_USART1,ENABLE);//串口端口使能 GPIO_InitStrue.GPIO_Mode=GPIO_Mode_AF_PP; GPIO_InitStrue.GPIO_Pin=GPIO_Pin_9; GPIO_InitStrue.GPIO_Speed=GPIO_Speed_10MHz; GPIO_Init(GPIOA,&amp;GPIO_InitStrue); GPIO_InitStrue.GPIO_Mode=GPIO_Mode_IN_FLOATING; GPIO_InitStrue.GPIO_Pin=GPIO_Pin_10; GPIO_InitStrue.GPIO_Speed=GPIO_Speed_10MHz; GPIO_Init(GPIOA,&amp;GPIO_InitStrue); USART_InitStrue.USART_BaudRate=115200; USART_InitStrue.USART_HardwareFlowControl=USART_HardwareFlowControl_None; USART_InitStrue.USART_Mode=USART_Mode_Tx|USART_Mode_Rx; USART_InitStrue.USART_Parity=USART_Parity_No; USART_InitStrue.USART_StopBits=USART_StopBits_1; USART_InitStrue.USART_WordLength=USART_WordLength_8b; USART_Init(USART1,&amp;USART_InitStrue); USART_Cmd(USART1,ENABLE);//使能串口1 USART_ITConfig(USART1,USART_IT_RXNE,ENABLE);//开启接收中断 NVIC_InitStrue.NVIC_IRQChannel=USART1_IRQn; NVIC_InitStrue.NVIC_IRQChannelCmd=ENABLE; NVIC_InitStrue.NVIC_IRQChannelPreemptionPriority=1; NVIC_InitStrue.NVIC_IRQChannelSubPriority=1; NVIC_Init(&amp;NVIC_InitStrue); &#125; void USART1_IRQHandler(void) &#123; u8 res; if(USART_GetITStatus(USART1,USART_IT_RXNE)!=RESET) &#123; res= USART_ReceiveData(USART1); USART_SendData(USART1,res); &#125; &#125; int main(void) &#123; NVIC_PriorityGroupConfig(NVIC_PriorityGroup_2); My_USART1_Init(); while(1); &#125; WiFi模块ESP8266 VCC连接正极(有些是3.3V,有些是5V)GND连接负极RXD:数据的接收端 (连接单片机或者USB转TTL模块的TXD)TXD:数据的发送端 (连接单片机或者USB转TTL模块的RXD) 参考博客 NRF24L01无线模块 NRF24L01是SPI 通信接口。 主要想实现的功能是——两个开发板通信，如果做图像处理，有时候一块开发板不够用，就需要两块开发板。开发板上 NRF24L01 模块接口和 STM32F4的连接： 8月4日学校负责老师突然让学生自己买器材，离比赛只剩两天半时间，服气。 云台在网上买了几个二维云台，用2个舵机控制。 比赛A题无线充电小车是去年省赛的题目，没想到会连续出两年，完全没准备，放弃。B题是巡线四旋翼，难，放弃。F题纸张计数，学校仪器设备不够，放弃。最后选了H题，电磁炮，正好用到了之前做滚球时的舵机和openmv。由于H题只要能成功发射电磁炮就能得十分，所以选这个人的题特别特别多，测评结束后我们学校获奖的六个组其中五个组是做电磁炮。只捡几个点写一下吧，毕竟最后只拿了省二~结果不尽人意，大神太多 电磁炮的炮管放在二维云台上控制电磁炮水平垂直运动，通过控制继电器高低电平控制充电时间，通过控制舵机旋转控制炮管角度，进而控制炮弹发射距离和方向。 按键模块使用的是手机APP，通过蓝牙模块HC-05向开发板（stm32F407核心板）传输发射距离和角度。 发挥部分的自动寻靶和测距用的是openmv，这些都能在openmv官网找到代码，openmv找到靶子并且测得距离后通过串口发送给开发板，继而控制舵机，然后发射炮弹。]]></content>
      <categories>
        <category>电子竞赛</category>
      </categories>
      <tags>
        <tag>电子竞赛</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux]]></title>
    <url>%2F2019%2F05%2F30%2FLinux%2F</url>
    <content type="text"><![CDATA[友链: 看完这篇Linux基本的操作就会了:https://zhuanlan.zhihu.com/p/36801617 Linux系统的组成： linux内核（linus 团队管理） shell：系统的用户界面，提供了用户与内核进行交互操作的一种接口(命令解释器) 文件系统：ext3、ext4等,windows 有 fat32 、ntfs 第三方应用软件 文件系统结构 bin 存放二进制可执行文件(ls,cat,mkdir等) boot 存放用于系统引导时使用的各种文件 dev 用于存放设备文件 etc 存放系统配置文件 home 存放所有用户文件的根目录 lib 存放跟文件系统中的程序运行所需要的共享库及内核模块 mnt 系统管理员安装临时文件系统的安装点 opt 额外安装的可选应用程序包所放置的位置 proc 虚拟文件系统，存放当前内存的映射 root 超级用户目录 sbin 存放二进制可执行文件，只有root才能访问 tmp 用于存放各种临时文件 usr 用于存放系统应用程序，比较重要的目录/usr/local 本地管理员软件安装目录 var 用于存放运行时需要改变数据的文件 通配符 *：匹配任何字符和任何数目的字符 ?：匹配单一数目的任何字符 [ ]：匹配[ ]之内的任意一个字符 [! ]：匹配除了[! ]之外的任意一个字符，!表示非的意思 文件的类型 普通文件- 目录d 符号链接l 硬链接：与普通文件没什么不同，inode 都指向同一个文件在硬盘中的区块 软链接：保存了其代表的文件的绝对路径，是另外一种文件，在硬盘上有独立的区块，访问时替换自身路径(简单地理解为 Windows 中常见的快捷方式)。 字符设备文件 c 块设备文件b 套接字s 命名管道p 常用的文件、目录操作命令 pwd命令查看用户的当前目录 cd 命令来切换目录 .表示当前目录 .. 表示当前目录的上一级目录（父目录） -表示用 cd 命令切换目录前所在的目录 ~ 表示用户主目录的绝对路径名 ls：显示文件或目录信息 mkdir：当前目录下创建一个空目录 rmdir：要求目录为空 touch：生成一个空文件或更改文件的时间(可以改变文件的三种时间，分别是： access time 、 modify time 、 change time) cp：复制文件或目录 mv：移动文件或目录、文件或目录改名 rm：删除文件或目录 ln：建立链接文件 find：查找文件 file/stat：查看文件类型或文件属性信息 cat：查看文本文件内容 more：可以分页看 less：不仅可以分页，还可以方便地搜索，回翻等操作 tail -10： 查看文件的尾部的10行 head -20：查看文件的头部20行 别名alias echo：把内容重定向到指定的文件中 ，有则打开，无则创建 管道命令 | ：将前面的结果给后面的命令，例如：ls -la | wc，将ls的结果加油wc命令来统计字数 重定向 &gt; 是覆盖模式，&gt;&gt; 是追加模式，例如：echo “Java3y,zhen de hen xi huan ni” &gt; qingshu.txt把左边的输出放到右边的文件里去 文件打包和压缩命令 压缩的方式也是有好几种，我们常用的有下面这三种： gzip bzip2 tar 常用的压缩的命令就有： gzip filename bzip2 filename tar -czvf filename 常用的解压命令有： gzip -d filename.gz bzip2 -d filename.bz2 tar -xzvf filename.tar.gz 正则表达式grep(global search regular expression)是一个强大的文本搜索工具。grep 使用正则表达式搜索文本，并把匹配的行打印出来。格式：grep [options] PATTERN [FILE…] PATTERN 是查找条件：可以是普通字符串、可以是正则表达式，通常用单引号将RE括起来。 FILE 是要查找的文件，可以是用空格间隔的多个文件，也可是使用Shell的通配符在多个文件中查找PATTERN，省略时表示在标准输入中查找。 grep命令不会对输入文件进行任何修改或影响，可以使用输出重定向将结果存为文件 例子： 在文件 myfile 中查找包含字符串 mystr的行 grep -n mystr myfile 显示 myfile 中第一个字符为字母的所有行 grep ‘^[a-zA-Z]’ myfile 在文件 myfile 中查找首字符不是 # 的行（即过滤掉注释行） grep -v ‘^#’ myfile 列出/etc目录（包括子目录）下所有文件内容中包含字符串“root”的文件名 grep -lr root /etc/* Shell变量 和 Shell环境Shell 变量大致可以分为三类： 内部变量：由系统提供，用户只能使用不能修改。 ? GROUPS 环境变量：这些变量决定了用户工作的环境，它们不需要用户去定义，可以直接在 shell 中使用，其中某些变量用户可以修改。 用户变量：由用户建立和修改，在 shell 脚本编写中会经常用到。 变量赋值（定义变量） varName=Value export varName=Value 引用变量$varName Shell变量的作用域： 局部变量的作用范围仅仅限制在其命令行所在的Shell或Shell脚本文件中； 全局变量的作用范围则包括本Shell进程及其所有子进程。 局部变量与全局变量互换：可以使用 export 内置命令将局部变量设置为全局变量。可以使用 export 内置命令将全局变量设置为局部变量。 export命令： 显示当前Shell可见的全局变量 export [-p] 定义变量值的同时声明为全局变量。 export &lt;变量名1=值1&gt; [&lt;变量名2=值2&gt; …] 声明已经赋值的某个（些）局部变量为全局变量。 export &lt;变量名1&gt; [&lt;变量名2&gt; …] 声明已经赋值的某个（些）全局变量为局部变量。 export -n &lt;变量名1&gt; [&lt;变量名2&gt; …] Shell环境变量： 环境变量定义 Shell 的运行环境，保证 Shell 命令的正确执行。 Shell用环境变量来确定查找路径、注册目录、终端类型、终端名称、用户名等。 所有环境变量都是全局变量（即可以传递给 Shell 的子进程），并可以由用户重新设置。 Shell变量：查询、显示和取消： 显示当前已经定义的所有变量 所有环境变量：env 所有变量和函数（包括环境变量） ：set 显示某（些）个变量的值 echo $NAME1 [$NAME2 ……] 取消变量的声明或赋值 unset VI编辑器vi 是 “Visual interface” 的简称，它可以执行输出、删除、查找、替换、块操作等众多文本操作，而且用户可以根据自己的需要对其进行定制，这是其他编辑程序所没有的。 普通模式 G用于直接跳转到文件尾 ZZ用于存盘退出Vi ZQ用于不存盘退出Vi /和？用于查找字符串 n继续查找下一个 yy复制一行 p粘帖在下一行，P粘贴在前一行 dd删除一行文本 x删除光标所在的字符 u取消上一次编辑操作（undo） 插入模式 在 Normal 模式下输入插入命令 i、 a 、 o进入insert模式。用户输入的任何字符都被vim当做文件内容保存起来，并将其显示在屏幕上。 在文本输入过程中，若想回到Normal模式下，按 Esc 键即可。 命令行模式 Normal 模式下，用户按冒号 :即可进入 Command 模式，此时 vim 会在显示窗口的最后一行 (屏幕的最后一行) 显示一个 “:” 作为 Command 模式的提示符，等待输入命令。 :w 保存当前编辑文件，但并不退出 :w newfile 存为另外一个名为 “newfile” 的文件 :wq 用于存盘退出Vi :q! 用于不存盘退出Vi :q用于直接退出Vi （未做修改） 设置Vi环境: :set autoindent 缩进,常用于程序的编写 :set noautoindent 取消缩进 :set number 在编辑文件时显示行号 :set nonumber 不显示行号 :set tabstop=value 设置显示制表符的空格字符个数 :set 显示设置的所有选项 :set all 显示所有可以设置的选项]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git]]></title>
    <url>%2F2019%2F05%2F29%2FGit%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[友链: git - 简易指南 廖雪峰教程 Git官网 Git必看书籍:Pro Git 很久之前在CSDN写的一篇 git入门博客 Git：分布式版本控制系统Git是版本控制系统，Github是在线的基于Git的代码托管服务。 基本的 Git 工作流程如下： 在工作目录中修改文件。 暂存文件，将文件的快照放入暂存区域。 提交更新，找到暂存区域的文件，将快照永久性存储到 Git 仓库目录。 版本控制是一种记录一个或若干文件内容变化，以便将来查阅特定版本修订情况的系统。 除了项目源代码，你可以对任何类型的文件进行版本控制。有了它你就可以将某个文件回溯到之前的状态，甚至将整个项目都回退到过去某个时间点的状态，你可以比较文件的变化细节，查出最后是谁修改了哪个地方，从而找出导致怪异问题出现的原因，又是谁在何时报告了某个功能缺陷等等。 Git采用的是直接记录快照的方式，而非差异比较。我后面会详细介绍这两种方式的差别。 集中式和分布式的区别集中式：CVS、SVN 集中式版本控制系统最大的毛病就是必须联网才能工作，如果在局域网内还好，带宽够大，速度够快，可如果在互联网上，遇到网速慢的话，可能提交一个10M的文件就需要5分钟。分布式版本控制系统根本没有“中央服务器”，每个人的电脑上都是一个完整的版本库，这样，你工作的时候，就不需要联网了，因为版本库就在你自己的电脑上。既然每个人电脑上都有一个完整的版本库。和集中式版本控制系统相比，分布式版本控制系统的安全性要高很多，因为每个人电脑里都有完整的版本库，某一个人的电脑坏掉了不要紧，随便从其他人那里复制一个就可以了。而集中式版本控制系统的中央服务器要是出了问题，所有人都没法干活了。 时光穿梭机版本库又名仓库，英文名repository，你可以简单理解成一个目录，这个目录里面的所有文件都可以被Git管理起来，每个文件的修改、删除，Git都能跟踪，以便任何时刻都可以追踪历史，或者在将来某个时刻可以“还原”。 1234$ mkdir learngit //选择一个合适的地方，创建一个空目录$ cd learngit$ pwd$ git init //这个目录变成Git可以管理的仓库 当前目录下多了一个.git的目录，这个目录是Git来跟踪管理版本库的，没事千万不要手动修改这个目录里面的文件，不然改乱了，就把Git仓库给破坏了。 Microsoft的Word格式是二进制格式，因此，版本控制系统是没法跟踪Word文件的改动的,如果要真正使用版本控制系统，就要以纯文本方式编写文件。因为文本是有编码的，比如中文有常用的GBK编码，日文有Shift_JIS编码，如果没有历史遗留问题，强烈建议使用标准的UTF-8编码，所有语言使用同一种编码，既没有冲突，又被所有平台所支持。 添加文件到Git仓库，分两步： 123$ git add readme.txt //注意，可反复多次使用，添加多个文件；$ git commit -m &quot;wrote a readme file&quot;$ cat readme.txt //读取文件内容 要随时掌握工作区的状态，使用git status命令。如果git status告诉你有文件被修改过，用git diff可以查看修改内容。在Git中，我们用git log命令查看。 123git statusgit diff readme.txtgit log 版本回退Git必须知道当前版本是哪个版本，在Git中，用HEAD表示当前版本，也就是最新的提交1094adb…（注意我的提交ID和你的肯定不一样），上一个版本就是HEAD^，上上一个版本就是HEAD^^，当然往上100个版本写100个^比较容易数不过来，所以写成HEAD~100。 123$ git reset --hard HEAD^$ git reset --hard deb9275$ git reflog 穿梭前，用git log可以查看提交历史，以便确定要回退到哪个版本。要重返未来，用git reflog查看命令历史，以便确定要回到未来的哪个版本。 工作区和暂存区工作区（Working Directory）,就是你在电脑里能看到的目录，比如我的learngit文件夹就是一个工作区。 工作区有一个隐藏目录.git，这个不算工作区，而是Git的版本库。Git的版本库里存了很多东西，其中最重要的就是称为stage（或者叫index）的暂存区，还有Git为我们自动创建的第一个分支master，以及指向master的一个指针叫HEAD。 Git 有三种状态——已提交（committed）、已修改（modified）和已暂存（staged） 第一步是用git add把文件添加进去，实际上就是把文件修改添加到暂存区；第二步是用git commit提交更改，实际上就是把暂存区的所有内容提交到当前分支。因为我们创建Git版本库时，Git自动为我们创建了唯一一个master分支，所以，现在，git commit就是往master分支上提交更改。 管理修改Git管理的是修改，而不是文件每次修改，如果不用git add到暂存区，那就不会加入到commit中。 撤销修改在准备提交前，猛然发现了错误,发现得很及时，就可以很容易地纠正它。你可以删掉最后一行，手动把文件恢复到上一个版本的状态。 1$ git checkout -- readme.txt 命令git checkout – readme.txt意思就是，把readme.txt文件在工作区的修改全部撤销，这里有两种情况：一种是readme.txt自修改后还没有被放到暂存区，现在，撤销修改就回到和版本库一模一样的状态；一种是readme.txt已经添加到暂存区后，又作了修改，现在，撤销修改就回到添加到暂存区后的状态。总之，就是让这个文件回到最近一次git commit或git add时的状态。 git checkout – file命令中的–很重要，没有–，就变成了“切换到另一个分支”的命令. 场景1：当你改乱了工作区某个文件的内容，想直接丢弃工作区的修改时，用命令git checkout – file。 场景2：当你不但改乱了工作区某个文件的内容，还添加到了暂存区时，想丢弃修改，分两步，第一步用命令git reset HEAD ，就回到了场景1，第二步按场景1操作。 场景3：已经提交了不合适的修改到版本库时，想要撤销本次提交，参考版本回退一节，不过前提是没有推送到远程库。 删除文件命令git rm用于删除一个文件。如果一个文件已经被提交到版本库，那么你永远不用担心误删，但是要小心，你只能恢复文件到最新版本，你会丢失最近一次提交后你修改的内容。 先添加一个新文件test.txt到Git并且提交，git add test.txt，git commit -m “add test.txt”，直接在文件管理器中把没用的文件删了： 1234567$ rm test.txt //工作区已经把该文件删除//下一步分为两种情况：//（1）如果删错了，因为版本库里还有呢，所以可以很轻松地把误删的文件恢复到最新版本：$ git checkout -- test.txt //注意：从来没有被添加到版本库就被删除的文件，是无法恢复的！//（2）确实要从版本库中删除该文件，那就用命令git rm删掉，并且git commit$ git rm test.txt$ git commit -m &quot;remove test.txt&quot; 远程仓库github：提供Git仓库托管服务的，所以，只要注册一个GitHub账号，就可以免费获得Git远程仓库。本地Git仓库和GitHub仓库之间的传输是通过SSH加密，具体做法参考我的csdn博客。 在本地创建了一个Git仓库后，又想在GitHub创建一个Git仓库，并且让这两个仓库进行远程同步，这样，GitHub上的仓库既可以作为备份，又可以让其他人通过该仓库来协作，操作步骤为： 123456git remote add origin https://github.com/swhaleDCC/learn.git//添加后，远程库的名字就是origin，这是Git默认的叫法，也可以改成别的，但是origin这个名字一看就知道是远程库。下一步，就可以把本地库的所有内容推送到远程库上：git push -u origin master//用git push命令，实际上是把当前分支master推送到远程,由于远程库是空的，我们第一次推送master分支时，加上了-u参数，Git不但会把本地的master分支内容推送的远程新的master分支，还会把本地的master分支和远程的master分支关联起来，在以后的推送或者拉取时就可以简化命令//从现在起，只要本地作了提交，就可以通过命令：git push origin master Git支持多种协议，包括https，但通过ssh支持的原生git协议速度最快。 分支管理创建与合并分支一开始的时候，master分支是一条线，Git用master指向最新的提交，再用HEAD指向master，就能确定当前分支，以及当前分支的提交点: 123456789101112131415161718192021222324//创建dev分支，然后切换到dev分支:$ git checkout -b dev//加上-b参数表示创建并切换，相当于以下两条命令：$ git branch dev$ git checkout dev//用git branch命令查看当前分支：$ git branch//然后，我们在dev分支上正常提交//现在，dev分支的工作完成，我们就可以切换回master分支：$ git checkout master//现在，我们把dev分支的工作成果合并到master分支上：$ git merge dev //git merge命令用于合并指定分支到当前分支//删除分支$ git branch -d dev//总结：查看分支：git branch创建分支：git branch &lt;name&gt;切换分支：git checkout &lt;name&gt;或者git switch &lt;name&gt;创建+切换分支：git checkout -b &lt;name&gt;或者git switch -c &lt;name&gt;合并某分支到当前分支：git merge &lt;name&gt;删除分支：git branch -d &lt;name&gt; 解决冲突当Git无法自动合并分支时，就必须首先解决冲突。解决冲突后，再提交，合并完成。git status可以告诉我们冲突的文件，Git用&lt;&lt;&lt;&lt;&lt;&lt;&lt;，=======，&gt;&gt;&gt;&gt;&gt;&gt;&gt;标记出不同分支的内容，我们修改后再提交，用带参数的git log可以看到分支的合并情况： 1$ git log --graph --pretty=oneline --abbrev-commit 解决冲突就是把Git合并失败的文件手动编辑为我们希望的内容，再提交。用git log –graph命令可以看到分支合并图。 分支管理策略–no-ff方式的git merge:通常，合并分支时，如果可能，Git会用Fast forward模式，但这种模式下，删除分支后，会丢掉分支信息。如果要强制禁用Fast forward模式，Git就会在merge时生成一个新的commit，这样，从分支历史上就可以看出分支信息。首先，master分支应该是非常稳定的，也就是仅用来发布新版本，平时不能在上面干活；干活都在dev分支上，也就是说，dev分支是不稳定的，到某个时候，比如1.0版本发布时，再把dev分支合并到master上，在master分支发布1.0版本；你和你的小伙伴们每个人都在dev分支上干活，每个人都有自己的分支，时不时地往dev分支上合并就可以了。合并分支时，加上–no-ff参数就可以用普通模式合并，合并后的历史有分支，能看出来曾经做过合并，而fast forward合并就看不出来曾经做过合并。 Bug分支修复bug时，我们会通过创建新的bug分支进行修复，然后合并，最后删除；当手头工作没有完成时，先把工作现场git stash一下，然后去修复bug，修复后，再git stash pop，回到工作现场；在master分支上修复的bug，想要合并到当前dev分支，可以用git cherry-pick 命令，把bug提交的修改“复制”到当前分支，避免重复劳动。 Feature分支开发一个新feature，最好新建一个分支；如果要丢弃一个没有被合并过的分支，可以通过git branch -D 强行删除。 多人协作小结： 查看远程库信息，使用git remote -v； 本地新建的分支如果不推送到远程，对其他人就是不可见的； 从本地推送分支，使用git push origin branch-name，如果推送失败，先用git pull抓取远程的新提交； 在本地创建和远程分支对应的分支，使用git checkout -b branch-name origin/branch-name，本地和远程分支的名称最好一致； 建立本地分支和远程分支的关联，使用git branch –set-upstream branch-name origin/branch-name； 从远程抓取分支，使用git pull，如果有冲突，要先处理冲突。Rebaserebase操作可以把本地未push的分叉提交历史整理成直线； rebase的目的是使得我们在查看历史提交的变化时更容易，因为分叉的提交需要三方对比。 标签管理创建标签1234$ git tag v1.0$ git tag //查看所有标签$ git show v1.0 //查看标签信息$ git tag -a v0.1 -m &quot;version 0.1 released&quot; 1094adb //创建带有说明的标签，用-a指定标签名，-m指定说明文字 默认标签是打在最新提交的commit上的。有时候，如果忘了打标签，比如，现在已经是周五了，但应该在周一打的标签没有打，怎么办？方法是找到历史提交的commit id，然后打上就可以了。 12$ git log --pretty=oneline --abbrev-commit$ git tag v0.9 f52c633 注意：标签总是和某个commit挂钩。如果这个commit既出现在master分支，又出现在dev分支，那么在这两个分支上都可以看到这个标签。 操作标签1234567$ git tag -d v0.1 //删除标签，因为创建的标签都只存储在本地，不会自动推送到远程。所以，打错的标签可以在本地安全删除$ git push origin &lt;tagname&gt; //推送某个标签到远程$ git push origin --tags //一次性推送全部尚未推送到远程的本地标签//如果标签已经推送到远程，要删除远程标签就麻烦一点，先从本地删除：$ git tag -d v0.9//然后，从远程删除。删除命令也是push，但是格式如下：$ git push origin :refs/tags/v0.9 码云国内的Git托管服务——码云 自定义Git]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
</search>
