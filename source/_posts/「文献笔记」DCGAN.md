title: 「文献笔记」DCGAN
author: Dccun
tags:
  - DCGAN
categories:
  - 深度学习
date: 2019-12-27 12:44:00
---
>2016-基于深度卷积对抗生成网络的无监督表示学习

<!--more-->

# abstract
- 近年来，CNNs的监督学习在计算机视觉应用中得到了广泛的应用。相对而言，使用CNNs进行无监督学习的研究较少。在这项工作中，我们希望能够帮助缩小CNNs在监督学习和非监督学习方面的成就的差距。我们介绍了一类称为深度卷积生成对抗网络(DCGANs)的CNNs，它具有一定的架构约束，并证明了它们是无监督学习的有力候选者。通过对各种图像数据集的训练，我们给出了令人信服的证据，证明我们的深卷积对偶在生成器和鉴别器中都学习了从对象部件到场景的表示层次。此外，我们将学习到的特性用于新任务——演示它们在一般图像上的适用性。

# introduction
- 我们提出了一种卷积GANs的架构，它能在大多数设置下稳定训练。我们把它命名为深度卷积GANs。
- 我们在图像分类任务上使用了预训练的分类器，来展示和其他无监督算法的竞争性。
- 我们可视化了GANs学习到的滤波器，并通过经验证明了特定的滤波器已经学会了绘制特定的图像。
- 我们证明了生成器具有有趣的向量算术属性，可以方便地操作生成的示例的许多语义质量。

# related work
- 无标签数据的表征学习：一个经典方法是对数据进行聚类(例如使用K-means)，并利用聚类来改进分类分数。在图像领域中，可以对图像块学习强大的图像表示。另一个流行的方法是训练自动编码器,编码一个图像到一个压缩码,编码和解码使重建图像尽可能准确。深度信念网络在学习层次表示方面也表现得很好。
- 生成自然图像：生成图像模型得到了广泛的研究，分为参数化和非参数化两大类。
- 可视化CNNs的内部结构：在CNNs的背景下，Zeiler等人通过反卷积和最大池化，可以找到网络中每个卷积滤波器的近似目的。类似地，对输入使用梯度下降让我们检查激活某些过滤器子集的理想图像(Mordvintsev等)。

# approach and model architecture
- 我们的方法的核心是采用和修改最近演示的对CNN架构的三个更改。
- 第一个是全卷积网络，它用`步长卷积`替换了确定性的空间池化函数(如maxpooling)，允许网络学习自己的空间下采样。我们在生成器中使用这种方法，允许它学习自己的空间上采样和鉴别器。
- 第二个是在卷积特征之上消除完全连接层的趋势。这方面最有力的例子是全局平均池化，它已被用于最先进的图像分类模型(Mordvintsev等)。我们发现，全局平均池化提高了模型的稳定性，但不利于收敛速度。将最高卷积特性直接连接到生成器和鉴别器的输入和输出的中间地带工作得很好。GAN的第一层以均匀的噪声分布Z作为输入，由于只是矩阵乘法，可以称为全连通，但结果被重新构造为一个四维张量，并作为卷积堆栈的起点。对于鉴别器，最后一个卷积层被平坦化，然后输入一个单一的sigmoid输出。
- 第三个是批归一化，通过将每个单元的输入归一化为零均值和单位方差来稳定学习。这有助于处理由于初始化不良而出现的训练问题，并有助于在更深层次的模型中实现梯度流。这对于让深层生成器开始学习非常关键，可以防止生成器将所有样本压缩到一个点，这是在GANs中观察到的常见故障模式。然而，直接将BatchNorm应用于所有层，会导致样品振荡和模型不稳定。通过不对生成器输出层和鉴别器输入层应用BatchNorm，避免了这种情况。除输出层使用Tanh函数外，生成器使用ReLU激活。我们观察到，使用有界激活可以使模型更快地学习饱和和覆盖训练分布的颜色空间。在鉴别器中，我们发现LeakyReLU激活效果很好，特别是对于高分辨率的建模。这与最初使用maxout激活的GAN论文形成了对比。
- 稳定的DCGAN结构指南
	- 用卷积和反卷积代替所有池化层
	- 使用批归一化
	- 为了更深的网络，移除了全连接隐层
	- 在生成器中使用ReLU激活函数（除了在输出的时候用了Tanh激活函数）
	- 在判别器中使用了LeakyReLU激活函数
    
# details of adversarial training
- 除了缩放到tanh激活函数的范围[-1,1]外，训练图像没有进行其他预处理。所有模型均采用mini-batch SGD进行训练，batch为128。所有的权值都是从0为中心的正态分布初始化的，标准差为0.02。在LeakyReLU中，所有模型的泄漏斜率都设置为0.2。以前的GAN工作使用动量来加速培训，而我们使用Adam优化器(Kingma & Ba, 2014)调优超参数。我们发现建议的学习率为0.001，用0.0002代替，太高了。此外,我们发现离开动量项β1的建议值为0.9时导致培训振荡和不稳定,同时减少到0.5帮助稳定训练。![upload successful](/images/pasted-79.png)
- 除了缩放到tanh激活函数的范围[-1,1]外，训练图像没有进行其他预处理。所有模型均采用mini-batch SGD进行训练，batch为128。所有的权值都是从0为中心的正态分布初始化的，标准差为0.02。在LeakyReLU中，所有模型的泄漏斜率都设置为0.2。以前的GAN工作使用动量来加速培训，而我们使用Adam优化器(Kingma & Ba, 2014)调优超参数。我们发现建议的学习率为0.001，用0.0002代替，太高了。此外,我们发现离开动量项β1的建议值为0.9时导致培训振荡和不稳定,同时减少到0.5帮助稳定训练。
- 在下面三个数据集训练DCGAN
	- LSUN （大规模场景理解）
    - Faces
    - Imagenet-1K

# empirical validation of DCGANs capabilities(实验验证DCGANS的能力)
- classifying `CIFAR-10` using GANs as a feature extractor（使用GANS作为特征提取器对CIFAR-10进行分类）
- classifying `SVHN` digits using GANs as a feature extractor（使用GANS作为特征提取器对SVHN数字进行分类）

# investigating and visualizing the internals of the networks(调查和可视化网络的内部结构)
- walking in the latent space（漫游隐空间）：各种各样的学习通常能告诉我们记忆的迹象（如果有急剧的转变）以及空间分层坍塌的方式。如果在这个潜在的空间中行走导致图像生成语义的变化（例如被添加和删除的对象），我们可以推断该模型已经学习了相关和有趣的表示。
- visualizing the discriminator features（可视化判别器特性）：以前的工作已经证明，在大型图像数据集上进行有监督的CNN训练会产生非常强大的学习功能。此外，在场景分类方面受监督的CNN学习对象检测。我们证明了在大图像数据集上训练无监督的DCGAN也可以学习有趣的特征层次。相比之下，在同一图中，我们为随机初始化特征给出了基线，这些特征在语义上相关或有趣的任何事物上不被激活。
- manipulating the generator representation（操纵生成器表示）
	- forgetting to draw certain objects（忘记画特定的对象）
    - vector arithmetic on face samples（向量算法的人脸样本）
    
# conclusion and future work

# supplementary material（补充材料）







