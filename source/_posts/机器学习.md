title: '[加密文章]机器学习深度学习笔记'
tags:
  - ML
  - DL
categories:
  - 深度学习
password: 199812
date: 2019-09-22 20:12:00
---
![upload successful](/images/pasted-0.png)
>友链:
- 吴恩达老师的机器学习课程个人笔记： https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes
- 吴恩达老师的深度学习课程笔记及资源： https://github.com/fengdu78/deeplearning_ai_books
- github上一个100天学习机器学习的项目：
	- https://github.com/MLEveryday/100-Days-Of-ML-Code
	- https://github.com/Avik-Jain/100-Days-Of-ML-Code

<!--more-->

# 深度学习框架
>全世界最为流行的深度学习框架有PaddlePaddle、Tensorflow、Caffe、Theano、MXNet、Torch和PyTorch。
conda是一个流行的Python包管理软件。
![upload successful](/images/pasted-1.png)

# 特征缩放
>特征缩放(feature scaling)
梯度下降算法中，在有多个特征的情况下，如果能确保这些不同的特征都处在一个相近的范围，这样梯度下降法就能更快地收敛。

# MNIST Dataset
>[MNIST Dataset Introduction](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/0_Prerequisite/mnist_dataset_intro.ipynb)
mnist数据集里的每张图片大小为28 * 28像素，可以用28*28的大小的数组来表示一张图片。
标签用大小为10的数组来表示，这种编码我们称之为One hot（独热编码）。

# One-hot编码
>One-hot编码（独热编码）
独热编码使用N位代表N种状态，任意时候只有其中一位有效。
采用独热编码的例子
性别:  
[0, 1]代表女，[1, 0]代表男
数字0-9: 
[0,0,0,0,0,0,0,0,0,1]代表9，[0,1,0,0,0,0,0,0,0,0]代表1

# 损失函数（loss function）
>常见的损失函数定义:
差的平方和 sum((y - label)^2)
交叉熵 -sum(label * log(y))(交叉熵只关注独热编码中有效位的损失)

# 梯度下降
![upload successful](/images/pasted-2.png)
learning_rate:学习速率

# softmax激活函数
>作用:一是放大效果，二是梯度下降时需要一个可导的函数。
```
def softmax(x):
    import numpy as np
    return np.exp(x) / np.sum(np.exp(x), axis=0)
softmax([4, 5, 10])
```

# tensorflow playground
附上一张我试着训练出来的loss最小的螺旋神经网络
![upload successful](/images/pasted-3.png)

# 混淆矩阵
混淆矩阵也称误差矩阵，是表示精度评价的一种标准格式，用n行n列的矩阵形式来表示。在人工智能中，混淆矩阵（confusion matrix）是可视化工具，特别用于监督学习，在无监督学习一般叫做匹配矩阵。

![upload successful](/images/pasted-11.png)

![upload successful](/images/pasted-12.png)

![upload successful](/images/pasted-13.png)

# [KL散度、JS散度、Wasserstein距离](https://www.cnblogs.com/jiangxinyang/p/10563113.html)

![upload successful](/images/pasted-119.png)

![upload successful](/images/pasted-120.png)

![upload successful](/images/pasted-121.png)

# [梯度下降法（BGD & SGD & Mini-batch SGD）](https://www.cnblogs.com/lvdongjie/p/11318008.html) 

`梯度下降法（Gradient Descent）`

优化思想：用当前位置的负梯度方向作为搜索方向，亦即为当前位置下降最快的方向。越接近目标值时，步长越小，下降越慢，梯度下降不一定能找到全局最优解，可能寻找到的是局部最优解。（当损失函数是凸函数时，梯度下降得到的解一定是全局最优解，因为凸函数的极小值即为最小值）
- 批量梯度下降法（Batch Gradient Descent，BGD）：每次是对整个训练集进行梯度下降。
- 随机梯度下降法（Stochastic Gradient Descent，SGD）：每次只对一个样本进行梯度下降。
- 小批量梯度下降法（Mini-batch Gradient Desent，也称Mini-batch SGD）：每次处理样本的个数在上面二者之间，把整个大的训练集划分为若干个小的训练集，在处理完整个训练集之前，先让梯度下降法处理一部分数据，那么算法就会相对快一些。当数据集很大时，使用Mini-batch SGD更新参数更快，有利于更鲁棒地收敛，避免局部最优。

`选择Mini-batch SGD的参数 batch size`

不难看出Mini-batch SGD的 batch 大小，也是一个影响着算法效率的参数。

如果训练集较小，一般小于2000的，就直接使用BGD。

一般Mini-batch SGD的大小在 64 到 512 之间，选择 2 的 n 次幂会运行得相对快一些。注意这个值设为 2 的 n 次幂，是为了符合cpu gpu的内存要求，如果不符合的话，不管用什么算法表现都会很糟糕。

[深度学习中的batch](https://www.zhihu.com/question/32673260/answer/71137399)

Batch_Size：批尺寸

![upload successful](/images/pasted-116.png)


# [Normalization模型](https://blog.csdn.net/malefactor/article/details/82154224)

　　在机器学习领域有个很重要的假设：`IID独立同分布假设`，也就是假设训练数据和测试数据是满足相同分布的，这是通过训练数据获得的模型能够在测试集上获得好的效果的一个基本保障。在深度学习网络中，后一层的输入是受前一层的影响的，而为了方便训练网络，我们一般都是采用Mini-Batch SGD来训练网络的（Mini-Batch SGD的两个优点是：梯度更新方向更准确和并行计算速度快）。

　　我们知道在神经网络训练开始前，都要对输入数据做一个归一化处理，那么具体为什么需要归一化呢？`归一化后有什么好处呢`？原因在于神经网络学习过程本质就是为了学习数据分布，一旦训练数据与测试数据的分布不同，那么网络的泛化能力也大大降低；另外一方面，一旦每批训练数据的分布各不相同(batch 梯度下降)，那么网络就要在每次迭代都去学习适应不同的分布，这样将会大大降低网络的训练速度，这也正是为什么我们需要对数据都要做一个归一化预处理的原因。

　　对于深度网络的训练是一个复杂的过程，只要网络的前面几层发生微小的改变，那么后面几层就会被累积放大下去。一旦网络某一层的输入数据的分布发生改变，那么这一层网络就需要去适应学习这个新的数据分布，所以如果训练过程中，训练数据的分布一直在发生变化，那么将会影响网络的训练速度。

　　除了输入层的数据外(因为输入层数据，我们已经人为的为每个样本归一化)，后面网络每一层的输入数据分布是一直在发生变化的，因为在训练的时候，前面层训练参数的更新将导致后面层输入数据分布的变化。以网络第二层为例：网络的第二层输入，是由第一层的参数和input计算得到的，而第一层的参数在整个训练过程中一直在变化，因此必然会引起后面每一层输入数据分布的改变。

　　我们把网络中间层在训练过程中，数据分布的改变称之为：“Internal  Covariate Shift”。Internal指的是深层网络的隐层，是发生在网络内部的事情，而不是covariate shift问题只发生在输入层。Batch Normalization就是来解决该问题的。Batch Normalization的基本思想就是能不能让每个隐层节点的激活输入分布固定下来，从而避免Internal Covariate Shift的问题。

　　BN(Batch Normalization)属于网络的一层。BN的基本思想其实相当直观：因为深层神经网络在做非线性变换前的激活输入值（就是那个x=WU+B，U是输入）随着网络深度加深或者在训练过程中，其分布逐渐发生偏移或者变动，之所以训练收敛慢，一般是整体分布逐渐往非线性函数的取值区间的上下限两端靠近（对于Sigmoid函数来说，意味着激活输入值WU+B是大的负值或正值），所以这导致后向传播时低层神经网络的梯度消失，这是训练深层神经网络收敛越来越慢的本质原因，而BN就是通过一定的规范化手段，`把每层神经网络任意神经元输入值的分布强行拉回到均值为0方差为1的标准正态分布，使得激活输入值落在非线性函数对输入比较敏感的区域，这样输入的小变化就会导致损失函数较大的变化，意思是这样让梯度变大，避免梯度消失问题产生，而且梯度变大意味着学习收敛速度快，能大大加快训练速度。`
  
# 理解dropout
参考：http://blog.csdn.net/stdcoutzyx/article/details/49022443

>dropout是指在深度学习网络的训练过程中，对于神经网络单元，按照一定的概率将其暂时从网络中丢弃。注意是暂时，对于随机梯度下降来说，由于是随机丢弃，故而每一个mini-batch都在训练不同的网络。

dropout率的选择：
- 经过交叉验证，隐含节点dropout率等于0.5的时候效果最好，原因是0.5的时候dropout随机生成的网络结构最多。
- dropout也可以被用作一种添加噪声的方法，直接对input进行操作。输入层设为更接近1的数。使得输入变化不会太大（0.8）













