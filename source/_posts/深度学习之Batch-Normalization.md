title: 深度学习之Batch Normalization
author: Dccun
tags:
  - BN
categories:
  - 深度学习
date: 2020-04-13 15:57:00
---
>友链：
- [深度学习中的Normalization模型](https://blog.csdn.net/malefactor/article/details/82154224)

<!--more-->

　　在机器学习领域有个很重要的假设：`IID独立同分布假设`，也就是假设训练数据和测试数据是满足相同分布的，这是通过训练数据获得的模型能够在测试集上获得好的效果的一个基本保障。在深度学习网络中，后一层的输入是受前一层的影响的，而为了方便训练网络，我们一般都是采用Mini-Batch SGD来训练网络的（Mini-Batch SGD的两个优点是：梯度更新方向更准确和并行计算速度快）。

　　我们知道在神经网络训练开始前，都要对输入数据做一个归一化处理，那么具体为什么需要归一化呢？`归一化后有什么好处呢`？原因在于神经网络学习过程本质就是为了学习数据分布，一旦训练数据与测试数据的分布不同，那么网络的泛化能力也大大降低；另外一方面，一旦每批训练数据的分布各不相同(batch 梯度下降)，那么网络就要在每次迭代都去学习适应不同的分布，这样将会大大降低网络的训练速度，这也正是为什么我们需要对数据都要做一个归一化预处理的原因。

　　对于深度网络的训练是一个复杂的过程，只要网络的前面几层发生微小的改变，那么后面几层就会被累积放大下去。一旦网络某一层的输入数据的分布发生改变，那么这一层网络就需要去适应学习这个新的数据分布，所以如果训练过程中，训练数据的分布一直在发生变化，那么将会影响网络的训练速度。

　　除了输入层的数据外(因为输入层数据，我们已经人为的为每个样本归一化)，后面网络每一层的输入数据分布是一直在发生变化的，因为在训练的时候，前面层训练参数的更新将导致后面层输入数据分布的变化。以网络第二层为例：网络的第二层输入，是由第一层的参数和input计算得到的，而第一层的参数在整个训练过程中一直在变化，因此必然会引起后面每一层输入数据分布的改变。

　　我们把网络中间层在训练过程中，数据分布的改变称之为：“Internal  Covariate Shift”。Internal指的是深层网络的隐层，是发生在网络内部的事情，而不是covariate shift问题只发生在输入层。Batch Normalization就是来解决该问题的。Batch Normalization的基本思想就是能不能让每个隐层节点的激活输入分布固定下来，从而避免Internal Covariate Shift的问题。

　　BN(Batch Normalization)属于网络的一层。BN的基本思想其实相当直观：因为深层神经网络在做非线性变换前的激活输入值（就是那个x=WU+B，U是输入）随着网络深度加深或者在训练过程中，其分布逐渐发生偏移或者变动，之所以训练收敛慢，一般是整体分布逐渐往非线性函数的取值区间的上下限两端靠近（对于Sigmoid函数来说，意味着激活输入值WU+B是大的负值或正值），所以这导致后向传播时低层神经网络的梯度消失，这是训练深层神经网络收敛越来越慢的本质原因，而BN就是通过一定的规范化手段，`把每层神经网络任意神经元输入值的分布强行拉回到均值为0方差为1的标准正态分布，使得激活输入值落在非线性函数对输入比较敏感的区域，这样输入的小变化就会导致损失函数较大的变化，意思是这样让梯度变大，避免梯度消失问题产生，而且梯度变大意味着学习收敛速度快，能大大加快训练速度。`
  
  
  
  
  