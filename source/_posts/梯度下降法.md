title: 梯度下降法（BGD & SGD & Mini-batch SGD）
author: Dccun
tags:
  - 梯度下降法
categories:
  - 深度学习
date: 2020-04-13 16:10:00
---
>友链：
- [梯度下降法（BGD & SGD & Mini-batch SGD）](https://www.cnblogs.com/lvdongjie/p/11318008.html) 

<!--more-->

# 梯度下降法（Gradient Descent）

优化思想：用当前位置的负梯度方向作为搜索方向，亦即为当前位置下降最快的方向。越接近目标值时，步长越小，下降越慢，梯度下降不一定能找到全局最优解，可能寻找到的是局部最优解。（当损失函数是凸函数时，梯度下降得到的解一定是全局最优解，因为凸函数的极小值即为最小值）
- 批量梯度下降法（Batch Gradient Descent，BGD）：每次是对整个训练集进行梯度下降。
- 随机梯度下降法（Stochastic Gradient Descent，SGD）：每次只对一个样本进行梯度下降。
- 小批量梯度下降法（Mini-batch Gradient Desent，也称Mini-batch SGD）：每次处理样本的个数在上面二者之间，把整个大的训练集划分为若干个小的训练集，在处理完整个训练集之前，先让梯度下降法处理一部分数据，那么算法就会相对快一些。当数据集很大时，使用Mini-batch SGD更新参数更快，有利于更鲁棒地收敛，避免局部最优。

# 选择Mini-batch SGD的参数 batch size
不难看出Mini-batch SGD的 batch 大小，也是一个影响着算法效率的参数。

如果训练集较小，一般小于2000的，就直接使用BGD。

一般Mini-batch SGD的大小在 64 到 512 之间，选择 2 的 n 次幂会运行得相对快一些。注意这个值设为 2 的 n 次幂，是为了符合cpu gpu的内存要求，如果不符合的话，不管用什么算法表现都会很糟糕。


